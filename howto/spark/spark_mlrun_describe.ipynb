{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Job with MLRun\n",
    "Basic read csv spark mlrun function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "import nuclio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datanode-registry.iguazio-platform.app.hsbctesting3.iguazio-cd0.com:80/iguazio/shell:3.0_katyak_debug_b1089_20201214154653'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get build base image name by running the following code\n",
    "# copy and paste the output of this cell into config spec.build.baseImage\n",
    "# this will use us to config our spark job docker image\n",
    "import os\n",
    "os.environ[\"IGZ_DATANODE_REGISTRY_URL\"] + '/iguazio/shell:' + os.environ[\"IGZ_VERSION\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%nuclio: setting kind to 'job'\n",
      "%nuclio: setting spec.build.baseImage to 'datanode-registry.iguazio-platform.app.hsbctesting3.iguazio-cd0.com:80/iguazio/shell:3.0_katyak_debug_b1089_20201214154653'\n"
     ]
    }
   ],
   "source": [
    "# set the function kind and docker image\n",
    "%nuclio config kind = \"job\"\n",
    "%nuclio config spec.build.baseImage = \"datanode-registry.iguazio-platform.app.hsbctesting3.iguazio-cd0.com:80/iguazio/shell:3.0_katyak_debug_b1089_20201214154653\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlrun\n",
    "from mlrun.platforms.iguazio import mount_v3io, mount_v3iod\n",
    "from mlrun.datastore import DataItem\n",
    "from mlrun.execution import MLClientCtx\n",
    "\n",
    "import os\n",
    "from subprocess import run\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql.types import LongType\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Spark Describe Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache using fc-list. This may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import base64 as b64\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from itertools import product\n",
    "import matplotlib\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from pkg_resources import resource_filename\n",
    "import six\n",
    "from pyspark.sql import DataFrame as SparkDataFrame\n",
    "from pyspark.sql.functions import (abs as df_abs, col, count, countDistinct,\n",
    "                                   max as df_max, mean, min as df_min,\n",
    "                                   sum as df_sum, when\n",
    "                                   )\n",
    "from pyspark.sql.functions import variance, stddev, kurtosis, skewness\n",
    "\n",
    "\n",
    "def describe(df, bins, corr_reject, config, **kwargs):\n",
    "    if not isinstance(df, SparkDataFrame):\n",
    "        raise TypeError(\"df must be of type pyspark.sql.DataFrame\")\n",
    "\n",
    "    # Number of rows:\n",
    "    table_stats = {\"n\": df.count()}\n",
    "    if table_stats[\"n\"] == 0:\n",
    "        raise ValueError(\"df cannot be empty\")\n",
    "\n",
    "    try:\n",
    "        matplotlib.style.use(\"default\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Function to \"pretty name\" floats:\n",
    "    def pretty_name(x):\n",
    "        x *= 100\n",
    "        if x == int(x):\n",
    "            return '%.0f%%' % x\n",
    "        else:\n",
    "            return '%.1f%%' % x\n",
    "\n",
    "    def corr_matrix(df, columns=None):\n",
    "        if columns is None:\n",
    "            columns = df.columns\n",
    "        combinations = list(product(columns,columns))\n",
    "\n",
    "        def separate(l, n):\n",
    "            for i in range(0, len(l), n):\n",
    "                yield l[i:i+n]\n",
    "\n",
    "        grouped = list(separate(combinations,len(columns)))\n",
    "        df_cleaned = df.select(*columns).na.drop(how=\"any\")\n",
    "\n",
    "        for i in grouped:\n",
    "            for j in enumerate(i):\n",
    "                i[j[0]] = i[j[0]] + (df_cleaned.corr(str(j[1][0]), str(j[1][1])),)\n",
    "\n",
    "        df_pandas = pd.DataFrame(grouped).applymap(lambda x: x[2])\n",
    "        df_pandas.columns = columns\n",
    "        df_pandas.index = columns\n",
    "        \n",
    "        return df_pandas\n",
    "\n",
    "    def create_hist_data(df, column, minim, maxim, bins=10):\n",
    "\n",
    "        def create_all_conditions(current_col, column, left_edges, count=1):\n",
    "            \"\"\"\n",
    "            Recursive function that exploits the\n",
    "            ability to call the Spark SQL Column method\n",
    "            .when() in a recursive way.\n",
    "            \"\"\"\n",
    "            left_edges = left_edges[:]\n",
    "            if len(left_edges) == 0:\n",
    "                return current_col\n",
    "            if len(left_edges) == 1:\n",
    "                next_col = current_col.when(col(column) >= float(left_edges[0]), count)\n",
    "                left_edges.pop(0)\n",
    "                return create_all_conditions(next_col, column, left_edges[:], count+1)\n",
    "            next_col = current_col.when((float(left_edges[0]) <= col(column))\n",
    "                                        & (col(column) < float(left_edges[1])), count)\n",
    "            left_edges.pop(0)\n",
    "            return create_all_conditions(next_col, column, left_edges[:], count+1)\n",
    "\n",
    "        num_range = maxim - minim\n",
    "        bin_width = num_range / float(bins)\n",
    "        left_edges = [minim]\n",
    "        for _bin in range(bins):\n",
    "            left_edges = left_edges + [left_edges[-1] + bin_width]\n",
    "        left_edges.pop()\n",
    "        expression_col = when((float(left_edges[0]) <= col(column))\n",
    "                              & (col(column) < float(left_edges[1])), 0)\n",
    "        left_edges_copy = left_edges[:]\n",
    "        left_edges_copy.pop(0)\n",
    "        bin_data = (df.select(col(column))\n",
    "                    .na.drop()\n",
    "                    .select(col(column),\n",
    "                            create_all_conditions(expression_col,\n",
    "                                                  column,\n",
    "                                                  left_edges_copy\n",
    "                                                 ).alias(\"bin_id\")\n",
    "                           )\n",
    "                    .groupBy(\"bin_id\").count()\n",
    "                   ).toPandas()\n",
    "\n",
    "        bin_data.index = bin_data[\"bin_id\"]\n",
    "        new_index = list(range(bins))\n",
    "        bin_data = bin_data.reindex(new_index)\n",
    "        bin_data[\"bin_id\"] = bin_data.index\n",
    "        bin_data = bin_data.fillna(0)\n",
    "\n",
    "        bin_data[\"left_edge\"] = left_edges\n",
    "        bin_data[\"width\"] = bin_width\n",
    "        \n",
    "\n",
    "        return bin_data\n",
    "\n",
    "\n",
    "    def describe_integer_1d(df, column, current_result, nrows):\n",
    "        \n",
    "        stats_df = df.select(column).na.drop().agg(mean(col(column)).alias(\"mean\"),\n",
    "                                                       df_min(col(column)).alias(\"min\"),\n",
    "                                                       df_max(col(column)).alias(\"max\"),\n",
    "                                                       variance(col(column)).alias(\"variance\"),\n",
    "                                                       kurtosis(col(column)).alias(\"kurtosis\"),\n",
    "                                                       stddev(col(column)).alias(\"std\"),\n",
    "                                                       skewness(col(column)).alias(\"skewness\"),\n",
    "                                                       df_sum(col(column)).alias(\"sum\")\n",
    "                                                       ).toPandas()\n",
    "\n",
    "\n",
    "        for x in np.array([0.05, 0.25, 0.5, 0.75, 0.95]):\n",
    "            stats_df[pretty_name(x)] = (df.select(column)\n",
    "                                        .na.drop()\n",
    "                                        .selectExpr(\"percentile(`{col}`,CAST({n} AS DOUBLE))\"\n",
    "                                                    .format(col=column, n=x)).toPandas().iloc[:,0]\n",
    "                                        )\n",
    "        stats = stats_df.iloc[0].copy()\n",
    "        stats.name = column\n",
    "        stats[\"range\"] = stats[\"max\"] - stats[\"min\"]\n",
    "        stats[\"iqr\"] = stats[pretty_name(0.75)] - stats[pretty_name(0.25)]\n",
    "        stats[\"cv\"] = stats[\"std\"] / float(stats[\"mean\"])\n",
    "        stats[\"mad\"] = (df.select(column)\n",
    "                        .na.drop()\n",
    "                        .select(df_abs(col(column)-stats[\"mean\"]).alias(\"delta\"))\n",
    "                        .agg(df_sum(col(\"delta\"))).toPandas().iloc[0,0] / float(current_result[\"count\"]))\n",
    "        stats[\"type\"] = \"NUM\"\n",
    "        stats['n_zeros'] = df.select(column).where(col(column)==0.0).count()\n",
    "        stats['p_zeros'] = stats['n_zeros'] / float(nrows)\n",
    "\n",
    "        hist_data = create_hist_data(df, column, stats[\"min\"], stats[\"max\"], bins)\n",
    "\n",
    "        return stats\n",
    "\n",
    "    def describe_float_1d(df, column, current_result, nrows):\n",
    "        stats_df = df.select(column).na.drop().agg(mean(col(column)).alias(\"mean\"),\n",
    "                                                       df_min(col(column)).alias(\"min\"),\n",
    "                                                       df_max(col(column)).alias(\"max\"),\n",
    "                                                       variance(col(column)).alias(\"variance\"),\n",
    "                                                       kurtosis(col(column)).alias(\"kurtosis\"),\n",
    "                                                       stddev(col(column)).alias(\"std\"),\n",
    "                                                       skewness(col(column)).alias(\"skewness\"),\n",
    "                                                       df_sum(col(column)).alias(\"sum\")\n",
    "                                                       ).toPandas()\n",
    "\n",
    "        for x in np.array([0.05, 0.25, 0.5, 0.75, 0.95]):\n",
    "            stats_df[pretty_name(x)] = (df.select(column)\n",
    "                                        .na.drop()\n",
    "                                        .selectExpr(\"percentile_approx(`{col}`,CAST({n} AS DOUBLE))\"\n",
    "                                                    .format(col=column, n=x)).toPandas().iloc[:,0]\n",
    "                                        )\n",
    "        stats = stats_df.iloc[0].copy()\n",
    "        stats.name = column\n",
    "        stats[\"range\"] = stats[\"max\"] - stats[\"min\"]\n",
    "        stats[\"iqr\"] = stats[pretty_name(0.75)] - stats[pretty_name(0.25)]\n",
    "        stats[\"cv\"] = stats[\"std\"] / float(stats[\"mean\"])\n",
    "        stats[\"mad\"] = (df.select(column)\n",
    "                        .na.drop()\n",
    "                        .select(df_abs(col(column)-stats[\"mean\"]).alias(\"delta\"))\n",
    "                        .agg(df_sum(col(\"delta\"))).toPandas().iloc[0,0] / float(current_result[\"count\"]))\n",
    "        stats[\"type\"] = \"NUM\"\n",
    "        stats['n_zeros'] = df.select(column).where(col(column)==0.0).count()\n",
    "        stats['p_zeros'] = stats['n_zeros'] / float(nrows)\n",
    "\n",
    "        hist_data = create_hist_data(df, column, stats[\"min\"], stats[\"max\"], bins)\n",
    "\n",
    "        return stats\n",
    "\n",
    "    def describe_date_1d(df, column):\n",
    "        stats_df = df.select(column).na.drop().agg(df_min(col(column)).alias(\"min\"),\n",
    "                                                   df_max(col(column)).alias(\"max\")\n",
    "                                                  ).toPandas()\n",
    "        stats = stats_df.iloc[0].copy()\n",
    "        stats.name = column\n",
    "\n",
    "        if isinstance(stats[\"max\"], pd.Timestamp):\n",
    "            stats = stats.astype(object)\n",
    "            stats[\"max\"] = str(stats[\"max\"].to_pydatetime())\n",
    "            stats[\"min\"] = str(stats[\"min\"].to_pydatetime())\n",
    "\n",
    "        else:\n",
    "            stats[\"range\"] = stats[\"max\"] - stats[\"min\"]\n",
    "        stats[\"type\"] = \"DATE\"\n",
    "        return stats\n",
    "\n",
    "    def guess_json_type(string_value):\n",
    "        try:\n",
    "            obj = json.loads(string_value)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "        return type(obj)\n",
    "\n",
    "    def describe_categorical_1d(df, column):\n",
    "        value_counts = (df.select(column).na.drop()\n",
    "                        .groupBy(column)\n",
    "                        .agg(count(col(column)))\n",
    "                        .orderBy(\"count({c})\".format(c=column),ascending=False)\n",
    "                       ).cache()\n",
    "\n",
    "        # Get the most frequent class:\n",
    "        stats = (value_counts\n",
    "                 .limit(1)\n",
    "                 .withColumnRenamed(column, \"top\")\n",
    "                 .withColumnRenamed(\"count({c})\".format(c=column), \"freq\")\n",
    "                ).toPandas().iloc[0]\n",
    "\n",
    "        top_50 = value_counts.limit(50).toPandas().sort_values(\"count({c})\".format(c=column),\n",
    "                                                               ascending=False)\n",
    "        top_50_categories = top_50[column].values.tolist()\n",
    "\n",
    "        others_count = pd.Series([df.select(column).na.drop()\n",
    "                        .where(~(col(column).isin(*top_50_categories)))\n",
    "                        .count()\n",
    "                        ], index=[\"***Other Values***\"])\n",
    "        others_distinct_count = pd.Series([value_counts\n",
    "                                .where(~(col(column).isin(*top_50_categories)))\n",
    "                                .count()\n",
    "                                ], index=[\"***Other Values Distinct Count***\"])\n",
    "\n",
    "        top = top_50.set_index(column)[\"count({c})\".format(c=column)]\n",
    "        top = top.append(others_count)\n",
    "        top = top.append(others_distinct_count)\n",
    "        stats[\"value_counts\"] = top\n",
    "        stats[\"type\"] = \"CAT\"\n",
    "        value_counts.unpersist()\n",
    "        unparsed_valid_jsons = df.select(column).na.drop().rdd.map(\n",
    "            lambda x: guess_json_type(x[column])).filter(\n",
    "            lambda x: x).distinct().collect()\n",
    "        stats[\"unparsed_json_types\"] = unparsed_valid_jsons\n",
    "        return stats\n",
    "\n",
    "    def describe_constant_1d(df, column):\n",
    "        stats = pd.Series(['CONST'], index=['type'], name=column)\n",
    "        stats[\"value_counts\"] = (df.select(column)\n",
    "                                 .na.drop()\n",
    "                                 .limit(1)).toPandas().iloc[:,0].value_counts()\n",
    "        return stats\n",
    "\n",
    "    def describe_unique_1d(df, column):\n",
    "        stats = pd.Series(['UNIQUE'], index=['type'], name=column)\n",
    "        stats[\"value_counts\"] = (df.select(column)\n",
    "                                 .na.drop()\n",
    "                                 .limit(50)).toPandas().iloc[:,0].value_counts()\n",
    "        return stats\n",
    "\n",
    "    def describe_1d(df, column, nrows, lookup_config=None):\n",
    "        column_type = df.select(column).dtypes[0][1]\n",
    "        if (\"array\" in column_type) or (\"stuct\" in column_type) or (\"map\" in column_type):\n",
    "            raise NotImplementedError(\"Column {c} is of type {t} and cannot be analyzed\".format(c=column, t=column_type))\n",
    "\n",
    "        distinct_count = df.select(column).agg(countDistinct(col(column)).alias(\"distinct_count\")).toPandas()\n",
    "        non_nan_count = df.select(column).na.drop().select(count(col(column)).alias(\"count\")).toPandas()\n",
    "        results_data = pd.concat([distinct_count, non_nan_count],axis=1)\n",
    "        results_data[\"p_unique\"] = results_data[\"distinct_count\"] / float(results_data[\"count\"])\n",
    "        results_data[\"is_unique\"] = results_data[\"distinct_count\"] == nrows\n",
    "        results_data[\"n_missing\"] = nrows - results_data[\"count\"]\n",
    "        results_data[\"p_missing\"] = results_data[\"n_missing\"] / float(nrows)\n",
    "        results_data[\"p_infinite\"] = 0\n",
    "        results_data[\"n_infinite\"] = 0\n",
    "        result = results_data.iloc[0].copy()\n",
    "        result[\"memorysize\"] = 0\n",
    "        result.name = column\n",
    "\n",
    "        if result[\"distinct_count\"] <= 1:\n",
    "            result = result.append(describe_constant_1d(df, column))\n",
    "        elif column_type in {\"tinyint\", \"smallint\", \"int\", \"bigint\"}:\n",
    "            result = result.append(describe_integer_1d(df, column, result, nrows))\n",
    "        elif column_type in {\"float\", \"double\", \"decimal\"}:\n",
    "            result = result.append(describe_float_1d(df, column, result, nrows))\n",
    "        elif column_type in {\"date\", \"timestamp\"}:\n",
    "            result = result.append(describe_date_1d(df, column))\n",
    "        elif result[\"is_unique\"] == True:\n",
    "            result = result.append(describe_unique_1d(df, column))\n",
    "        else:\n",
    "            result = result.append(describe_categorical_1d(df, column))\n",
    "            # Fix to also count MISSING value in the distict_count field:\n",
    "            if result[\"n_missing\"] > 0:\n",
    "                result[\"distinct_count\"] = result[\"distinct_count\"] + 1\n",
    "\n",
    "        if (result[\"count\"] > result[\"distinct_count\"] > 1):\n",
    "            try:\n",
    "                result[\"mode\"] = result[\"top\"]\n",
    "            except KeyError:\n",
    "                result[\"mode\"] = 0\n",
    "        else:\n",
    "            try:\n",
    "                result[\"mode\"] = result[\"value_counts\"].index[0]\n",
    "            except KeyError:\n",
    "                result[\"mode\"] = 0\n",
    "            # If and IndexError happens,\n",
    "            # it is because all column are NULLs:\n",
    "            except IndexError:\n",
    "                result[\"mode\"] = \"MISSING\"\n",
    "\n",
    "        if lookup_config:\n",
    "            lookup_object = lookup_config['object']\n",
    "            col_name_in_db = lookup_config['col_name_in_db'] if 'col_name_in_db' in lookup_config else None\n",
    "            try:\n",
    "                matched, unmatched = lookup_object.lookup(df.select(column), col_name_in_db)\n",
    "                result['lookedup_values'] = str(matched.count()) + \"/\" + str(df.select(column).count())\n",
    "            except:\n",
    "                result['lookedup_values'] = 'FAILED'\n",
    "        else:\n",
    "            result['lookedup_values'] = ''\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "    # Do the thing:\n",
    "    ldesc = {}\n",
    "    for colum in df.columns:\n",
    "        if colum in config:\n",
    "            if 'lookup' in config[colum]:\n",
    "                lookup_config = config[colum]['lookup']\n",
    "                desc = describe_1d(df, colum, table_stats[\"n\"], lookup_config=lookup_config)\n",
    "            else:\n",
    "                desc = describe_1d(df, colum, table_stats[\"n\"])\n",
    "        else:\n",
    "            desc = describe_1d(df, colum, table_stats[\"n\"])\n",
    "        ldesc.update({colum: desc})\n",
    "\n",
    "    # Compute correlation matrix\n",
    "    if corr_reject is not None:\n",
    "        computable_corrs = [colum for colum in ldesc if ldesc[colum][\"type\"] in {\"NUM\"}]\n",
    "\n",
    "        if len(computable_corrs) > 0:\n",
    "            corr = corr_matrix(df, columns=computable_corrs)\n",
    "            for x, corr_x in corr.iterrows():\n",
    "                for y, corr in corr_x.iteritems():\n",
    "                    if x == y:\n",
    "                        break\n",
    "\n",
    "    # Convert ldesc to a DataFrame\n",
    "    variable_stats = pd.DataFrame(ldesc)\n",
    "\n",
    "    # General statistics\n",
    "    table_stats[\"nvar\"] = len(df.columns)\n",
    "    table_stats[\"total_missing\"] = float(variable_stats.loc[\"n_missing\"].sum()) / (table_stats[\"n\"] * table_stats[\"nvar\"])\n",
    "    memsize = 0\n",
    "    table_stats['memsize'] = fmt_bytesize(memsize)\n",
    "    table_stats['recordsize'] = fmt_bytesize(memsize / table_stats['n'])\n",
    "    table_stats.update({k: 0 for k in (\"NUM\", \"DATE\", \"CONST\", \"CAT\", \"UNIQUE\", \"CORR\")})\n",
    "    table_stats.update(dict(variable_stats.loc['type'].value_counts()))\n",
    "    table_stats['REJECTED'] = table_stats['CONST'] + table_stats['CORR']\n",
    "\n",
    "    freq_dict = {}\n",
    "    for var in variable_stats:\n",
    "        if \"value_counts\" not in variable_stats[var]:\n",
    "            pass\n",
    "        elif not(variable_stats[var][\"value_counts\"] is np.nan):\n",
    "            freq_dict[var] = variable_stats[var][\"value_counts\"]\n",
    "        else:\n",
    "            pass\n",
    "    try:\n",
    "        variable_stats = variable_stats.drop(\"value_counts\")\n",
    "    except (ValueError, KeyError):\n",
    "        pass\n",
    "\n",
    "    return table_stats, variable_stats.T, freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.functions import abs as absou\n",
    "\n",
    "SKEWNESS_CUTOFF = 20\n",
    "DEFAULT_FLOAT_FORMATTER = u'spark_df_profiling.__default_float_formatter'\n",
    "\n",
    "\n",
    "def gradient_format(value, limit1, limit2, c1, c2):\n",
    "    def LerpColour(c1,c2,t):\n",
    "        return (int(c1[0]+(c2[0]-c1[0])*t),int(c1[1]+(c2[1]-c1[1])*t),int(c1[2]+(c2[2]-c1[2])*t))\n",
    "    c = LerpColour(c1, c2, (value-limit1)/(limit2-limit1))\n",
    "    return fmt_color(value,\"rgb{}\".format(str(c)))\n",
    "\n",
    "\n",
    "def fmt_color(text, color):\n",
    "    return(u'<span style=\"color:{color}\">{text}</span>'.format(color=color,text=str(text)))\n",
    "\n",
    "\n",
    "def fmt_class(text, cls):\n",
    "    return(u'<span class=\"{cls}\">{text}</span>'.format(cls=cls,text=str(text)))\n",
    "\n",
    "\n",
    "def fmt_bytesize(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if num < 0:\n",
    "            num = num*-1\n",
    "            if num < 1024.0:\n",
    "                return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "            num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "\n",
    "def fmt_percent(v):\n",
    "    return  \"{:2.1f}%\".format(v*100)\n",
    "\n",
    "def fmt_varname(v):\n",
    "    return u'<code>{0}</code>'.format(v)\n",
    "\n",
    "\n",
    "value_formatters={\n",
    "        u'freq': (lambda v: gradient_format(v, 0, 62000, (30, 198, 244), (99, 200, 72))),\n",
    "        u'p_missing': fmt_percent,\n",
    "        u'p_infinite': fmt_percent,\n",
    "        u'p_unique': fmt_percent,\n",
    "        u'p_zeros': fmt_percent,\n",
    "        u'memorysize': fmt_bytesize,\n",
    "        u'total_missing': fmt_percent,\n",
    "        DEFAULT_FLOAT_FORMATTER: lambda v: str(float('{:.5g}'.format(v))).rstrip('0').rstrip('.'),\n",
    "        u'correlation_var': lambda v: fmt_varname(v),\n",
    "        u'unparsed_json_types': lambda v: ', '.join([s.__name__ for s in v])\n",
    "        }\n",
    "\n",
    "def fmt_row_severity(v):\n",
    "    if np.isnan(v) or v<= 0.01:\n",
    "        return \"ignore\"\n",
    "    else:\n",
    "        return \"alert\"\n",
    "\n",
    "def fmt_skewness(v):\n",
    "    if not np.isnan(v) and (v<-SKEWNESS_CUTOFF or v> SKEWNESS_CUTOFF):\n",
    "        return \"alert\"\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "row_formatters={\n",
    "    u'p_zeros': fmt_row_severity,\n",
    "    u'p_missing': fmt_row_severity,\n",
    "    u'p_infinite': fmt_row_severity,\n",
    "    u'n_duplicates': fmt_row_severity,\n",
    "    u'skewness': fmt_skewness,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Spark Describe Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "run([\"/bin/bash\", \"/etc/config/v3io/spark-job-init.sh\"])\n",
    "\n",
    "def describe_spark(context: MLClientCtx, \n",
    "                   dataset: DataItem,\n",
    "                   bins: int=30,\n",
    "                   describe_extended: bool=True)-> None:\n",
    "    \"\"\"\n",
    "    Generates profile reports from an Apache Spark DataFrame. \n",
    "    Based on pandas_profiling, but for Spark's DataFrames instead of pandas.\n",
    "    For each column the following statistics - if relevant for the column type - are presented:\n",
    "    \n",
    "    Essentials: type, unique values, missing values\n",
    "    \n",
    "    Quantile statistics: minimum value, Q1, median, Q3, maximum, range, interquartile range\n",
    "    \n",
    "    Descriptive statistics: mean, mode, standard deviation, sum, median absolute deviation, \n",
    "                            coefficient of variation, kurtosis, skewness\n",
    "                            \n",
    "    Most frequent values: for categorical data \n",
    "    --------------------------------------------------------------------------------------------\n",
    "    Parameters:\n",
    "                context : MLClientCtx\n",
    "                          MLRun introduces a concept of a runtime \"context\", \n",
    "                          the code can be set up to get parameters and inputs from the context, \n",
    "                          as well as log run outputs, artifacts, tags, and time-series metrics in the context.\n",
    "                                      \n",
    "                dataset : csv_file\n",
    "                          csv file which needs to be local (on our machine)\n",
    "                          the default location will be \"/v3io/projects/<file_name> \n",
    "                          which can be change by using mlrun.mount_v3io later in the function specs\n",
    "                          \n",
    "                bins :    Integer\n",
    "                          Number of bin in histograms\n",
    "                          \n",
    "                describe_extended : Bool \n",
    "                         (True) set to False if the aim is to get a simple \n",
    "                         pandas.DataFrame.describe() like infomration\n",
    "    ---------------------------------------------------------------------------------------------\n",
    "    Examples: \n",
    "               run mlrun function example, inputs will be part of the function inputs.\n",
    "               artifact_path is part of mlrun function parameters which set the path \n",
    "               for logging artifacts, results, dataset, etc.\n",
    "               \n",
    "               function.run(inputs={\"dataset\": \"iris.csv\",\n",
    "                                    \"bins\": 30,\n",
    "                                    \"describe_extended\": True},\n",
    "                                     artifact_path=artifact_path)\n",
    "    \"\"\"\n",
    "    \n",
    "    # get file location\n",
    "    location = dataset.local()\n",
    "    \n",
    "    # build spark session\n",
    "    spark = SparkSession.builder.appName(\"Spark job\").config(\"spark.executor.memory\",\"6g\").getOrCreate()\n",
    "    \n",
    "    # read csv\n",
    "    df = spark.read.csv(location, header=True, inferSchema= True)\n",
    "\n",
    "    # No use for now\n",
    "    kwargs = []\n",
    "    \n",
    "    # take only numric column\n",
    "    float_cols = [item[0] for item in df.dtypes if item[1].startswith('float') or item[1].startswith('double')]\n",
    "    \n",
    "    if describe_extended == True:\n",
    "        \n",
    "        # run describe function\n",
    "        table, variables, freq = describe(df, bins, float_cols, kwargs)\n",
    "\n",
    "        # get summary table\n",
    "        tbl_1 = variables.reset_index()\n",
    "\n",
    "        # prep report \n",
    "        if len(freq) != 0:\n",
    "            tbl_2 = pd.DataFrame.from_dict(freq, orient = \"index\").sort_index().stack().reset_index()\n",
    "            tbl_2.columns = ['col', 'key', 'val']\n",
    "            tbl_2['Merged'] = [{key: val} for key, val in zip(tbl_2.key, tbl_2.val)]\n",
    "            tbl_2 = tbl_2.groupby('col', as_index=False).agg(lambda x: tuple(x))[['col','Merged']]\n",
    "\n",
    "            # get summary\n",
    "            summary = pd.merge(tbl_1, tbl_2, how='left', left_on='index', right_on='col')\n",
    "\n",
    "        else:\n",
    "            summary = tbl_1\n",
    "\n",
    "        # log final report\n",
    "        context.log_dataset(\"summary_stats\", \n",
    "                            df=summary,\n",
    "                            format=\"csv\", index=False,\n",
    "                            artifact_path=context.artifact_subpath('data'))\n",
    "\n",
    "        # log overview\n",
    "        context.log_results(table)\n",
    "    \n",
    "    else:\n",
    "        # run simple describe and save to pandas\n",
    "        tbl_1 = df.describe().toPandas()\n",
    "        \n",
    "        # save final report and transpose \n",
    "        summary = tbl_1.T\n",
    "        \n",
    "        # log final report\n",
    "        context.log_dataset(\"summary_stats\", \n",
    "                            df=summary,\n",
    "                            format=\"csv\", index=False,\n",
    "                            artifact_path=context.artifact_subpath('data'))\n",
    "    \n",
    "    # stop spark session\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please don't remove the # nuclio: end-code cell above\n",
    "### Set MLRun Function Specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save spark service name (based on Iguazio services dashboard)\n",
    "spark_service = \"spark\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlrun will transform the code above (up to nuclio: end-code cell) into serverless function \n",
    "# which will run in k8s pods\n",
    "fn = mlrun.code_to_function(handler=\"describe_spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply mount_v3io over our function so that our k8s pod which run our function\n",
    "# will be able to access our data (shared data access)\n",
    "fn.apply(mlrun.mount_v3io_extended())\n",
    "fn.apply(mount_v3iod(namespace=\"default-tenant\", v3io_config_configmap=spark_service + \"-submit\"))\n",
    "\n",
    "# skip pulling an image if it already exists. If you would like to always force a pull, \n",
    "# you can set the imagePullPolicy of the container to Always.\n",
    "fn.spec.image_pull_policy = \"IfNotPresent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add build commands to our docker image with required moduls\n",
    "fn.spec.build.commands = ['pip install matplotlib mlrun==0.6.0-rc6 pyspark']\n",
    "\n",
    "# sets environment param in our docker image\n",
    "fn.spec.build.extra = 'ENV PATH $PATH:/igz/.local/bin'\n",
    "\n",
    "# print(fn.to_yamel()) will allow you to see the full function specs\n",
    "# addtional specs available and current specs applied to the current function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2020-12-21 07:29:32,981 [info] starting remote build, image: .mlrun/func-default-describe-spark-withui-latest\n",
      "\u001b[36mINFO\u001b[0m[0020] Retrieving image manifest datanode-registry.iguazio-platform.app.hsbctesting3.iguazio-cd0.com:80/iguazio/shell:3.0_katyak_debug_b1089_20201214154653 \n",
      "\u001b[36mINFO\u001b[0m[0020] Retrieving image manifest datanode-registry.iguazio-platform.app.hsbctesting3.iguazio-cd0.com:80/iguazio/shell:3.0_katyak_debug_b1089_20201214154653 \n",
      "\u001b[36mINFO\u001b[0m[0020] Built cross stage deps: map[]                \n",
      "\u001b[36mINFO\u001b[0m[0020] Retrieving image manifest datanode-registry.iguazio-platform.app.hsbctesting3.iguazio-cd0.com:80/iguazio/shell:3.0_katyak_debug_b1089_20201214154653 \n",
      "\u001b[36mINFO\u001b[0m[0020] Retrieving image manifest datanode-registry.iguazio-platform.app.hsbctesting3.iguazio-cd0.com:80/iguazio/shell:3.0_katyak_debug_b1089_20201214154653 \n",
      "\u001b[36mINFO\u001b[0m[0020] Executing 0 build triggers                   \n",
      "\u001b[36mINFO\u001b[0m[0020] Unpacking rootfs as cmd RUN pip install matplotlib mlrun==0.6.0-rc6 pyspark requires it. \n",
      "\u001b[36mINFO\u001b[0m[0098] RUN pip install matplotlib mlrun==0.6.0-rc6 pyspark \n",
      "\u001b[36mINFO\u001b[0m[0098] Taking snapshot of full filesystem...        \n",
      "\u001b[36mINFO\u001b[0m[0174] cmd: /bin/sh                                 \n",
      "\u001b[36mINFO\u001b[0m[0174] args: [-c pip install matplotlib mlrun==0.6.0-rc6 pyspark] \n",
      "\u001b[36mINFO\u001b[0m[0174] util.Lookup returned: &{Uid:1000 Gid:1000 Username:iguazio Name: HomeDir:/igz} \n",
      "\u001b[36mINFO\u001b[0m[0174] performing slow lookup of group ids for iguazio \n",
      "\u001b[36mINFO\u001b[0m[0174] Running: [/bin/sh -c pip install matplotlib mlrun==0.6.0-rc6 pyspark] \n",
      "WARNING: The directory '/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.3.3-cp37-cp37m-manylinux1_x86_64.whl (11.6 MB)\n",
      "Collecting mlrun==0.6.0-rc6\n",
      "  Downloading mlrun-0.6.0rc6-py3-none-any.whl (290 kB)\n",
      "Requirement already satisfied: pyspark in /spark/python (3.0.0)\n",
      "Collecting python-dateutil>=2.1\n",
      "  Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Collecting numpy>=1.15\n",
      "  Downloading numpy-1.19.4-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n",
      "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3\n",
      "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.3.1-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-8.0.1-cp37-cp37m-manylinux1_x86_64.whl (2.2 MB)\n",
      "Collecting pandas~=1.0\n",
      "  Downloading pandas-1.1.5-cp37-cp37m-manylinux1_x86_64.whl (9.5 MB)\n",
      "Collecting alembic~=1.4\n",
      "  Downloading alembic-1.4.3-py2.py3-none-any.whl (159 kB)\n",
      "Collecting semver~=2.13\n",
      "  Downloading semver-2.13.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting v3io~=0.5.0\n",
      "  Downloading v3io-0.5.6-py3-none-any.whl (49 kB)\n",
      "Collecting humanfriendly~=8.2\n",
      "  Downloading humanfriendly-8.2-py2.py3-none-any.whl (86 kB)\n",
      "Collecting kfp~=1.0.1\n",
      "  Downloading kfp-1.0.4.tar.gz (116 kB)\n",
      "Collecting fastapi~=0.62.0\n",
      "  Downloading fastapi-0.62.0-py3-none-any.whl (49 kB)\n",
      "Collecting google-auth<2.0dev,>=1.19.1\n",
      "  Downloading google_auth-1.24.0-py2.py3-none-any.whl (114 kB)\n",
      "Collecting nuclio-jupyter>=0.8.8\n",
      "  Downloading nuclio_jupyter-0.8.9-py3-none-any.whl (47 kB)\n",
      "Collecting boto3~=1.9\n",
      "  Downloading boto3-1.16.40-py2.py3-none-any.whl (130 kB)\n",
      "Collecting v3io-frames~=0.8.5\n",
      "  Downloading v3io_frames-0.8.8-py3-none-any.whl (35 kB)\n",
      "Collecting pyyaml~=5.1\n",
      "  Downloading PyYAML-5.3.1.tar.gz (269 kB)\n",
      "Collecting ipython<7.17,>=5.5\n",
      "  Downloading ipython-7.16.1-py3-none-any.whl (785 kB)\n",
      "Collecting requests~=2.22\n",
      "  Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
      "Collecting azure-storage-blob~=12.0\n",
      "  Downloading azure_storage_blob-12.6.0-py2.py3-none-any.whl (328 kB)\n",
      "Collecting GitPython~=3.0\n",
      "  Downloading GitPython-3.1.11-py3-none-any.whl (159 kB)\n",
      "Collecting aiohttp~=3.6\n",
      "  Downloading aiohttp-3.7.3-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
      "Collecting click~=7.0\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "Collecting pyarrow~=2.0\n",
      "  Downloading pyarrow-2.0.0-cp37-cp37m-manylinux2014_x86_64.whl (17.7 MB)\n",
      "Collecting kubernetes~=11.0\n",
      "  Downloading kubernetes-11.0.0-py3-none-any.whl (1.5 MB)\n",
      "Collecting mergedeep~=1.3\n",
      "  Downloading mergedeep-1.3.0-py3-none-any.whl (6.3 kB)\n",
      "Collecting orjson<3.4,>=3\n",
      "  Downloading orjson-3.3.1-cp37-cp37m-manylinux2014_x86_64.whl (208 kB)\n",
      "Collecting pydantic~=1.5\n",
      "  Downloading pydantic-1.7.3-cp37-cp37m-manylinux2014_x86_64.whl (9.1 MB)\n",
      "Collecting tabulate<=0.8.3,>=0.8.0\n",
      "  Downloading tabulate-0.8.3.tar.gz (46 kB)\n",
      "Collecting nest-asyncio~=1.0\n",
      "  Downloading nest_asyncio-1.4.3-py3-none-any.whl (5.3 kB)\n",
      "Collecting urllib3<1.27,>=1.25.4\n",
      "  Downloading urllib3-1.26.2-py2.py3-none-any.whl (136 kB)\n",
      "Collecting dask~=2.12\n",
      "  Downloading dask-2.30.0-py3-none-any.whl (848 kB)\n",
      "Collecting sqlalchemy~=1.3\n",
      "  Downloading SQLAlchemy-1.3.22-cp37-cp37m-manylinux2010_x86_64.whl (1.3 MB)\n",
      "Collecting py4j==0.10.9\n",
      "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
      "Requirement already satisfied: six>=1.5 in /conda/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib) (1.12.0)\n",
      "Collecting pytz>=2017.2\n",
      "  Downloading pytz-2020.4-py2.py3-none-any.whl (509 kB)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.1.3-py2.py3-none-any.whl (75 kB)\n",
      "Collecting python-editor>=0.3\n",
      "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
      "Collecting future>=0.18.2\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "Collecting ujson>=3.0.0\n",
      "  Downloading ujson-4.0.1-cp37-cp37m-manylinux1_x86_64.whl (179 kB)\n",
      "Collecting google-cloud-storage>=1.13.0\n",
      "  Downloading google_cloud_storage-1.35.0-py2.py3-none-any.whl (96 kB)\n",
      "Collecting requests_toolbelt>=0.8.0\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "Collecting cloudpickle\n",
      "  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
      "Collecting kfp-server-api<2.0.0,>=0.2.5\n",
      "  Downloading kfp-server-api-1.2.0.tar.gz (54 kB)\n",
      "Collecting jsonschema>=3.0.1\n",
      "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "Collecting Deprecated\n",
      "  Downloading Deprecated-1.2.10-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting strip-hints\n",
      "  Downloading strip-hints-0.1.9.tar.gz (30 kB)\n",
      "Collecting starlette==0.13.6\n",
      "  Downloading starlette-0.13.6-py3-none-any.whl (59 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading rsa-4.6-py3-none-any.whl (47 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.19.1->mlrun==0.6.0-rc6) (41.0.0)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.0-py3-none-any.whl (12 kB)\n",
      "Collecting nbconvert>=5.4\n",
      "  Downloading nbconvert-6.0.7-py3-none-any.whl (552 kB)\n",
      "Collecting notebook>=5.2.0\n",
      "  Downloading notebook-6.1.5-py3-none-any.whl (9.5 MB)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
      "Collecting botocore<1.20.0,>=1.19.40\n",
      "  Downloading botocore-1.19.40-py2.py3-none-any.whl (7.1 MB)\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting googleapis-common-protos>=1.5.3\n",
      "  Downloading googleapis_common_protos-1.52.0-py2.py3-none-any.whl (100 kB)\n",
      "Collecting grpcio-tools==1.30.0\n",
      "  Downloading grpcio_tools-1.30.0-cp37-cp37m-manylinux2010_x86_64.whl (2.5 MB)\n",
      "Collecting grpcio==1.30.0\n",
      "  Downloading grpcio-1.30.0-cp37-cp37m-manylinux2010_x86_64.whl (3.0 MB)\n",
      "Collecting pygments\n",
      "  Downloading Pygments-2.7.3-py3-none-any.whl (950 kB)\n",
      "Collecting decorator\n",
      "  Downloading decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
      "Collecting traitlets>=4.2\n",
      "  Downloading traitlets-5.0.5-py3-none-any.whl (100 kB)\n",
      "Collecting pickleshare\n",
      "  Downloading pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB)\n",
      "Collecting backcall\n",
      "  Downloading backcall-0.2.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting pexpect; sys_platform != \"win32\"\n",
      "  Downloading pexpect-4.8.0-py2.py3-none-any.whl (59 kB)\n",
      "Collecting jedi>=0.10\n",
      "  Downloading jedi-0.17.2-py2.py3-none-any.whl (1.4 MB)\n",
      "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
      "  Downloading prompt_toolkit-3.0.8-py3-none-any.whl (355 kB)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /conda/lib/python3.7/site-packages (from requests~=2.22->mlrun==0.6.0-rc6) (2.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /conda/lib/python3.7/site-packages (from requests~=2.22->mlrun==0.6.0-rc6) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /conda/lib/python3.7/site-packages (from requests~=2.22->mlrun==0.6.0-rc6) (2020.12.5)\n",
      "Collecting azure-core<2.0.0,>=1.9.0\n",
      "  Downloading azure_core-1.9.0-py2.py3-none-any.whl (124 kB)\n",
      "Requirement already satisfied: cryptography>=2.1.4 in /conda/lib/python3.7/site-packages (from azure-storage-blob~=12.0->mlrun==0.6.0-rc6) (2.6.1)\n",
      "Collecting msrest>=0.6.10\n",
      "  Downloading msrest-0.6.19-py2.py3-none-any.whl (84 kB)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.5-py3-none-any.whl (63 kB)\n",
      "Collecting typing-extensions>=3.6.5\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n",
      "Collecting attrs>=17.3.0\n",
      "  Downloading attrs-20.3.0-py2.py3-none-any.whl (49 kB)\n",
      "Collecting async-timeout<4.0,>=3.0\n",
      "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
      "Collecting requests-oauthlib\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0\n",
      "  Downloading websocket_client-0.57.0-py2.py3-none-any.whl (200 kB)\n",
      "Collecting MarkupSafe>=0.9.2\n",
      "  Downloading MarkupSafe-1.1.1-cp37-cp37m-manylinux1_x86_64.whl (27 kB)\n",
      "Collecting google-resumable-media<2.0dev,>=1.2.0\n",
      "  Downloading google_resumable_media-1.2.0-py2.py3-none-any.whl (75 kB)\n",
      "Collecting google-cloud-core<2.0dev,>=1.4.1\n",
      "  Downloading google_cloud_core-1.5.0-py2.py3-none-any.whl (27 kB)\n",
      "Collecting importlib-metadata; python_version < \"3.8\"\n",
      "  Downloading importlib_metadata-3.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting pyrsistent>=0.14.0\n",
      "  Downloading pyrsistent-0.17.3.tar.gz (106 kB)\n",
      "Collecting wrapt<2,>=1.10\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Requirement already satisfied: wheel in /conda/lib/python3.7/site-packages (from strip-hints->kfp~=1.0.1->mlrun==0.6.0-rc6) (0.33.1)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting mistune<2,>=0.8.1\n",
      "  Downloading mistune-0.8.4-py2.py3-none-any.whl (16 kB)\n",
      "Collecting jupyterlab-pygments\n",
      "  Downloading jupyterlab_pygments-0.1.2-py2.py3-none-any.whl (4.6 kB)\n",
      "Collecting jupyter-core\n",
      "  Downloading jupyter_core-4.7.0-py3-none-any.whl (82 kB)\n",
      "Collecting defusedxml\n",
      "  Downloading defusedxml-0.6.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting testpath\n",
      "  Downloading testpath-0.4.4-py2.py3-none-any.whl (163 kB)\n",
      "Collecting pandocfilters>=1.4.1\n",
      "  Downloading pandocfilters-1.4.3.tar.gz (16 kB)\n",
      "Collecting jinja2>=2.4\n",
      "  Downloading Jinja2-2.11.2-py2.py3-none-any.whl (125 kB)\n",
      "Collecting nbformat>=4.4\n",
      "  Downloading nbformat-5.0.8-py3-none-any.whl (172 kB)\n",
      "Collecting bleach\n",
      "  Downloading bleach-3.2.1-py2.py3-none-any.whl (145 kB)\n",
      "Collecting entrypoints>=0.2.2\n",
      "  Downloading entrypoints-0.3-py2.py3-none-any.whl (11 kB)\n",
      "Collecting nbclient<0.6.0,>=0.5.0\n",
      "  Downloading nbclient-0.5.1-py3-none-any.whl (65 kB)\n",
      "Collecting pyzmq>=17\n",
      "  Downloading pyzmq-20.0.0-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\n",
      "Collecting argon2-cffi\n",
      "  Downloading argon2_cffi-20.1.0-cp35-abi3-manylinux1_x86_64.whl (97 kB)\n",
      "Collecting ipython-genutils\n",
      "  Downloading ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting tornado>=5.0\n",
      "  Downloading tornado-6.1-cp37-cp37m-manylinux2010_x86_64.whl (428 kB)\n",
      "Collecting Send2Trash\n",
      "  Downloading Send2Trash-1.5.0-py3-none-any.whl (12 kB)\n",
      "Collecting terminado>=0.8.3\n",
      "  Downloading terminado-0.9.1-py3-none-any.whl (13 kB)\n",
      "Collecting jupyter-client>=5.3.4\n",
      "  Downloading jupyter_client-6.1.7-py3-none-any.whl (108 kB)\n",
      "Collecting prometheus-client\n",
      "  Downloading prometheus_client-0.9.0-py2.py3-none-any.whl (53 kB)\n",
      "Collecting ipykernel\n",
      "  Downloading ipykernel-5.4.2-py3-none-any.whl (119 kB)\n",
      "Collecting protobuf>=3.6.0\n",
      "  Downloading protobuf-3.14.0-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n",
      "Collecting ptyprocess>=0.5\n",
      "  Downloading ptyprocess-0.6.0-py2.py3-none-any.whl (39 kB)\n",
      "Collecting parso<0.8.0,>=0.7.0\n",
      "  Downloading parso-0.7.1-py2.py3-none-any.whl (109 kB)\n",
      "Collecting wcwidth\n",
      "  Downloading wcwidth-0.2.5-py2.py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in /conda/lib/python3.7/site-packages (from cryptography>=2.1.4->azure-storage-blob~=12.0->mlrun==0.6.0-rc6) (0.24.0)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /conda/lib/python3.7/site-packages (from cryptography>=2.1.4->azure-storage-blob~=12.0->mlrun==0.6.0-rc6) (1.12.2)\n",
      "Collecting isodate>=0.6.0\n",
      "  Downloading isodate-0.6.0-py2.py3-none-any.whl (45 kB)\n",
      "Collecting smmap<4,>=3.0.1\n",
      "  Downloading smmap-3.0.4-py2.py3-none-any.whl (25 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0; python_version >= \"3.5\"\n",
      "  Downloading google_crc32c-1.1.0-cp37-cp37m-manylinux2010_x86_64.whl (39 kB)\n",
      "Collecting google-api-core<2.0.0dev,>=1.21.0\n",
      "  Downloading google_api_core-1.24.1-py2.py3-none-any.whl (92 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.4.0-py3-none-any.whl (5.2 kB)\n",
      "Collecting webencodings\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Collecting packaging\n",
      "  Downloading packaging-20.8-py2.py3-none-any.whl (39 kB)\n",
      "Collecting async-generator\n",
      "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: pycparser in /conda/lib/python3.7/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.1.4->azure-storage-blob~=12.0->mlrun==0.6.0-rc6) (2.19)\n",
      "Building wheels for collected packages: kfp, pyyaml, tabulate, future, kfp-server-api, strip-hints, pyrsistent, wrapt, pandocfilters\n",
      "  Building wheel for kfp (setup.py): started\n",
      "  Building wheel for kfp (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp: filename=kfp-1.0.4-py3-none-any.whl size=159872 sha256=c12f80077fb518ae12cd72bd32977df3674b4dc9780f2e8aea56882cb278f708\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-0nbvqkai/wheels/65/1c/be/3d7366d2288bf1587e4fe6cd0c1ebdce5e3bada21b70a29e66\n",
      "  Building wheel for pyyaml (setup.py): started\n",
      "  Building wheel for pyyaml (setup.py): finished with status 'done'\n",
      "  Created wheel for pyyaml: filename=PyYAML-5.3.1-cp37-cp37m-linux_x86_64.whl size=44620 sha256=2f87b269d80fea10b48d972bb142498f54677d7b9da5935a5f50f23b2a06edfc\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-0nbvqkai/wheels/5e/03/1e/e1e954795d6f35dfc7b637fe2277bff021303bd9570ecea653\n",
      "  Building wheel for tabulate (setup.py): started\n",
      "  Building wheel for tabulate (setup.py): finished with status 'done'\n",
      "  Created wheel for tabulate: filename=tabulate-0.8.3-py3-none-any.whl size=23377 sha256=5a9a31d13328d8d4a67ccb7ee7f0085537f9813a4dbbfe2f8c4d48febab1c0b3\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-0nbvqkai/wheels/b8/a2/a6/812a8a9735b090913e109133c7c20aaca4cf07e8e18837714f\n",
      "  Building wheel for future (setup.py): started\n",
      "  Building wheel for future (setup.py): finished with status 'done'\n",
      "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491056 sha256=11c82a99ced51212f0a640428a115dcfed3ab82ba8a36f9c4ff47e8cf7f5ee60\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-0nbvqkai/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
      "  Building wheel for kfp-server-api (setup.py): started\n",
      "  Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-1.2.0-py3-none-any.whl size=108010 sha256=f20d32887e72b7c7c6a5ceecd7339c005a8e4755dd08c643581be93d458c5901\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-0nbvqkai/wheels/dd/25/e5/fa739adb2dfb29e0d1a5e2b71cccd4993c8db6c58112f3ba88\n",
      "  Building wheel for strip-hints (setup.py): started\n",
      "  Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "  Created wheel for strip-hints: filename=strip_hints-0.1.9-py2.py3-none-any.whl size=20993 sha256=6369fc5d78ae8363ff8e988cd8919455d8f1dcc2e95daaba0ad5ccee3931ce29\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-0nbvqkai/wheels/2d/b8/4e/a3ec111d2db63cec88121bd7c0ab1a123bce3b55dd19dda5c1\n",
      "  Building wheel for pyrsistent (setup.py): started\n",
      "  Building wheel for pyrsistent (setup.py): finished with status 'done'\n",
      "  Created wheel for pyrsistent: filename=pyrsistent-0.17.3-cp37-cp37m-linux_x86_64.whl size=123635 sha256=a7604e42006451a038fa1b7627de56467145b45fac8c077b2d933464cd45642f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-0nbvqkai/wheels/a5/52/bf/71258a1d7b3c8cbe1ee53f9314c6f65f20385481eaee573cc5\n",
      "  Building wheel for wrapt (setup.py): started\n",
      "  Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=76413 sha256=cf325f2546160742ba81a0bc4f93336cab829bd1f4c482e61c9000a008f118aa\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-0nbvqkai/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
      "  Building wheel for pandocfilters (setup.py): started\n",
      "  Building wheel for pandocfilters (setup.py): finished with status 'done'\n",
      "  Created wheel for pandocfilters: filename=pandocfilters-1.4.3-py3-none-any.whl size=7992 sha256=1756348b0f5487da6b1021a13571798254d60fe6cf32d8db83666320c474dceb\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-0nbvqkai/wheels/42/81/34/545dc2fbf0e9137811e901108d37fc04650e81d48f97078000\n",
      "Successfully built kfp pyyaml tabulate future kfp-server-api strip-hints pyrsistent wrapt pandocfilters\n",
      "Installing collected packages: python-dateutil, cycler, numpy, pyparsing, kiwisolver, pillow, matplotlib, pytz, pandas, MarkupSafe, Mako, python-editor, sqlalchemy, alembic, semver, urllib3, requests, future, ujson, v3io, humanfriendly, pyyaml, google-crc32c, google-resumable-media, pyasn1, rsa, pyasn1-modules, cachetools, google-auth, protobuf, googleapis-common-protos, google-api-core, google-cloud-core, google-cloud-storage, oauthlib, requests-oauthlib, websocket-client, kubernetes, requests-toolbelt, cloudpickle, kfp-server-api, zipp, typing-extensions, importlib-metadata, pyrsistent, attrs, jsonschema, tabulate, click, wrapt, Deprecated, strip-hints, kfp, starlette, pydantic, fastapi, jmespath, botocore, s3transfer, boto3, mistune, pygments, jupyterlab-pygments, ipython-genutils, traitlets, jupyter-core, defusedxml, testpath, pandocfilters, jinja2, nbformat, webencodings, packaging, bleach, entrypoints, async-generator, tornado, pyzmq, jupyter-client, nest-asyncio, nbclient, nbconvert, argon2-cffi, Send2Trash, ptyprocess, terminado, prometheus-client, decorator, pickleshare, backcall, pexpect, parso, jedi, wcwidth, prompt-toolkit, ipython, ipykernel, notebook, nuclio-jupyter, grpcio, grpcio-tools, v3io-frames, azure-core, isodate, msrest, azure-storage-blob, smmap, gitdb, GitPython, multidict, yarl, async-timeout, aiohttp, pyarrow, mergedeep, orjson, dask, mlrun, py4j\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.24.1\n",
      "    Uninstalling urllib3-1.24.1:\n",
      "      Successfully uninstalled urllib3-1.24.1\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.21.0\n",
      "    Uninstalling requests-2.21.0:\n",
      "      Successfully uninstalled requests-2.21.0\n",
      "ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "google-api-core 1.24.1 requires six>=1.13.0, but you'll have six 1.12.0 which is incompatible.\n",
      "Successfully installed Deprecated-1.2.10 GitPython-3.1.11 Mako-1.1.3 MarkupSafe-1.1.1 Send2Trash-1.5.0 aiohttp-3.7.3 alembic-1.4.3 argon2-cffi-20.1.0 async-generator-1.10 async-timeout-3.0.1 attrs-20.3.0 azure-core-1.9.0 azure-storage-blob-12.6.0 backcall-0.2.0 bleach-3.2.1 boto3-1.16.40 botocore-1.19.40 cachetools-4.2.0 click-7.1.2 cloudpickle-1.6.0 cycler-0.10.0 dask-2.30.0 decorator-4.4.2 defusedxml-0.6.0 entrypoints-0.3 fastapi-0.62.0 future-0.18.2 gitdb-4.0.5 google-api-core-1.24.1 google-auth-1.24.0 google-cloud-core-1.5.0 google-cloud-storage-1.35.0 google-crc32c-1.1.0 google-resumable-media-1.2.0 googleapis-common-protos-1.52.0 grpcio-1.30.0 grpcio-tools-1.30.0 humanfriendly-8.2 importlib-metadata-3.3.0 ipykernel-5.4.2 ipython-7.16.1 ipython-genutils-0.2.0 isodate-0.6.0 jedi-0.17.2 jinja2-2.11.2 jmespath-0.10.0 jsonschema-3.2.0 jupyter-client-6.1.7 jupyter-core-4.7.0 jupyterlab-pygments-0.1.2 kfp-1.0.4 kfp-server-api-1.2.0 kiwisolver-1.3.1 kubernetes-11.0.0 matplotlib-3.3.3 mergedeep-1.3.0 mistune-0.8.4 mlrun-0.6.0rc6 msrest-0.6.19 multidict-5.1.0 nbclient-0.5.1 nbconvert-6.0.7 nbformat-5.0.8 nest-asyncio-1.4.3 notebook-6.1.5 nuclio-jupyter-0.8.9 numpy-1.19.4 oauthlib-3.1.0 orjson-3.3.1 packaging-20.8 pandas-1.1.5 pandocfilters-1.4.3 parso-0.7.1 pexpect-4.8.0 pickleshare-0.7.5 pillow-8.0.1 prometheus-client-0.9.0 prompt-toolkit-3.0.8 protobuf-3.14.0 ptyprocess-0.6.0 py4j-0.10.9 pyarrow-2.0.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pydantic-1.7.3 pygments-2.7.3 pyparsing-2.4.7 pyrsistent-0.17.3 python-dateutil-2.8.1 python-editor-1.0.4 pytz-2020.4 pyyaml-5.3.1 pyzmq-20.0.0 requests-2.25.1 requests-oauthlib-1.3.0 requests-toolbelt-0.9.1 rsa-4.6 s3transfer-0.3.3 semver-2.13.0 smmap-3.0.4 sqlalchemy-1.3.22 starlette-0.13.6 strip-hints-0.1.9 tabulate-0.8.3 terminado-0.9.1 testpath-0.4.4 tornado-6.1 traitlets-5.0.5 typing-extensions-3.7.4.3 ujson-4.0.1 urllib3-1.26.2 v3io-0.5.6 v3io-frames-0.8.8 wcwidth-0.2.5 webencodings-0.5.1 websocket-client-0.57.0 wrapt-1.12.1 yarl-1.6.3 zipp-3.4.0\n",
      "\u001b[36mINFO\u001b[0m[0212] Taking snapshot of full filesystem...        \n",
      "\u001b[36mINFO\u001b[0m[0236] ENV PATH $PATH:/igz/.local/bin               \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build and deploy our docker image\n",
    "fn.deploy(with_mlrun=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set MLRun and Run Function\n",
    "Once running the function get be monitored here and our projects dashbaord<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set mlrun api path and arrtifact path for logging\n",
    "artifact_path = mlrun.set_environment(api_path = 'http://mlrun-api:8080',\n",
    "                                      artifact_path = os.path.abspath('./'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2020-12-21 07:40:12,571 [info] starting run describe-spark-withui-describe_spark uid=88288fb691df46f9874c0a86eedd1700 DB=http://mlrun-api:8080\n",
      "> 2020-12-21 07:40:12,725 [info] Job is running in the background, pod: describe-spark-withui-describe-spark-bfcvw\n",
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-8dgdj4xf because the default path (/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n",
      "Matplotlib is building the font cache; this may take a moment.\n",
      "20/12/21 07:41:35 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "> 2020-12-21 07:42:24,239 [info] run executed, status=completed\n",
      "final state: completed                                                          \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dictlist {\n",
       "  background-color: #4EC64B;\n",
       "  text-align: center;\n",
       "  margin: 4px;\n",
       "  border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;}\n",
       ".artifact {\n",
       "  cursor: pointer;\n",
       "  background-color: #4EC64B;\n",
       "  text-align: left;\n",
       "  margin: 4px; border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;\n",
       "}\n",
       "div.block.hidden {\n",
       "  display: none;\n",
       "}\n",
       ".clickable {\n",
       "  cursor: pointer;\n",
       "}\n",
       ".ellipsis {\n",
       "  display: inline-block;\n",
       "  max-width: 60px;\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "}\n",
       ".master-wrapper {\n",
       "  display: flex;\n",
       "  flex-flow: row nowrap;\n",
       "  justify-content: flex-start;\n",
       "  align-items: stretch;\n",
       "}\n",
       ".master-tbl {\n",
       "  flex: 3\n",
       "}\n",
       ".master-wrapper > div {\n",
       "  margin: 4px;\n",
       "  padding: 10px;\n",
       "}\n",
       "iframe.fileview {\n",
       "  border: 0 none;\n",
       "  height: 100%;\n",
       "  width: 100%;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       ".pane-header-title {\n",
       "  width: 80%;\n",
       "  font-weight: 500;\n",
       "}\n",
       ".pane-header {\n",
       "  line-height: 1;\n",
       "  background-color: #4EC64B;\n",
       "  padding: 3px;\n",
       "}\n",
       ".pane-header .close {\n",
       "  font-size: 20px;\n",
       "  font-weight: 700;\n",
       "  float: right;\n",
       "  margin-top: -5px;\n",
       "}\n",
       ".master-wrapper .right-pane {\n",
       "  border: 1px inset silver;\n",
       "  width: 40%;\n",
       "  min-height: 300px;\n",
       "  flex: 3\n",
       "  min-width: 500px;\n",
       "}\n",
       ".master-wrapper * {\n",
       "  box-sizing: border-box;\n",
       "}\n",
       "</style><script>\n",
       "function copyToClipboard(fld) {\n",
       "    if (document.queryCommandSupported && document.queryCommandSupported('copy')) {\n",
       "        var textarea = document.createElement('textarea');\n",
       "        textarea.textContent = fld.innerHTML;\n",
       "        textarea.style.position = 'fixed';\n",
       "        document.body.appendChild(textarea);\n",
       "        textarea.select();\n",
       "\n",
       "        try {\n",
       "            return document.execCommand('copy'); // Security exception may be thrown by some browsers.\n",
       "        } catch (ex) {\n",
       "\n",
       "        } finally {\n",
       "            document.body.removeChild(textarea);\n",
       "        }\n",
       "    }\n",
       "}\n",
       "function expandPanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName');\n",
       "  console.log(el.title);\n",
       "\n",
       "  document.querySelector(panelName + \"-title\").innerHTML = el.title\n",
       "  iframe = document.querySelector(panelName + \"-body\");\n",
       "\n",
       "  const tblcss = `<style> body { font-family: Arial, Helvetica, sans-serif;}\n",
       "    #csv { margin-bottom: 15px; }\n",
       "    #csv table { border-collapse: collapse;}\n",
       "    #csv table td { padding: 4px 8px; border: 1px solid silver;} </style>`;\n",
       "\n",
       "  function csvToHtmlTable(str) {\n",
       "    return '<div id=\"csv\"><table><tr><td>' +  str.replace(/[\\n\\r]+$/g, '').replace(/[\\n\\r]+/g, '</td></tr><tr><td>')\n",
       "      .replace(/,/g, '</td><td>') + '</td></tr></table></div>';\n",
       "  }\n",
       "\n",
       "  function reqListener () {\n",
       "    if (el.title.endsWith(\".csv\")) {\n",
       "      iframe.setAttribute(\"srcdoc\", tblcss + csvToHtmlTable(this.responseText));\n",
       "    } else {\n",
       "      iframe.setAttribute(\"srcdoc\", this.responseText);\n",
       "    }\n",
       "    console.log(this.responseText);\n",
       "  }\n",
       "\n",
       "  const oReq = new XMLHttpRequest();\n",
       "  oReq.addEventListener(\"load\", reqListener);\n",
       "  oReq.open(\"GET\", el.title);\n",
       "  oReq.send();\n",
       "\n",
       "\n",
       "  //iframe.src = el.title;\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.remove(\"hidden\");\n",
       "  }\n",
       "}\n",
       "function closePanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName')\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (!resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.add(\"hidden\");\n",
       "  }\n",
       "}\n",
       "\n",
       "</script>\n",
       "<div class=\"master-wrapper\">\n",
       "  <div class=\"block master-tbl\"><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>project</th>\n",
       "      <th>uid</th>\n",
       "      <th>iter</th>\n",
       "      <th>start</th>\n",
       "      <th>state</th>\n",
       "      <th>name</th>\n",
       "      <th>labels</th>\n",
       "      <th>inputs</th>\n",
       "      <th>parameters</th>\n",
       "      <th>results</th>\n",
       "      <th>artifacts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>default</td>\n",
       "      <td><div title=\"88288fb691df46f9874c0a86eedd1700\"><a href=\"https://mlrun-ui.default-tenant.app.hsbctesting3.iguazio-cd0.com/projects/default/jobs/monitor/88288fb691df46f9874c0a86eedd1700/info\" target=\"_blank\" >...eedd1700</a></div></td>\n",
       "      <td>0</td>\n",
       "      <td>Dec 21 07:41:33</td>\n",
       "      <td>completed</td>\n",
       "      <td>describe-spark-withui-describe_spark</td>\n",
       "      <td><div class=\"dictlist\">v3io_user=admin</div><div class=\"dictlist\">kind=job</div><div class=\"dictlist\">owner=admin</div><div class=\"dictlist\">host=describe-spark-withui-describe-spark-bfcvw</div></td>\n",
       "      <td><div class=\"artifact\" onclick=\"expandPanel(this)\" paneName=\"result6c98f502\" title=\"/files/iris.csv\">dataset</div></td>\n",
       "      <td></td>\n",
       "      <td><div class=\"dictlist\">n=150</div><div class=\"dictlist\">nvar=5</div><div class=\"dictlist\">total_missing=0.0</div><div class=\"dictlist\">memsize=0.0 YiB</div><div class=\"dictlist\">recordsize=0.0 YiB</div><div class=\"dictlist\">NUM=5</div><div class=\"dictlist\">DATE=0</div><div class=\"dictlist\">CONST=0</div><div class=\"dictlist\">CAT=0</div><div class=\"dictlist\">UNIQUE=0</div><div class=\"dictlist\">CORR=0</div><div class=\"dictlist\">REJECTED=0</div></td>\n",
       "      <td><div class=\"artifact\" onclick=\"expandPanel(this)\" paneName=\"result6c98f502\" title=\"/files/data/summary_stats.csv\">summary_stats</div></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></div>\n",
       "  <div id=\"result6c98f502-pane\" class=\"right-pane block hidden\">\n",
       "    <div class=\"pane-header\">\n",
       "      <span id=\"result6c98f502-title\" class=\"pane-header-title\">Title</span>\n",
       "      <span onclick=\"closePanel(this)\" paneName=\"result6c98f502\" class=\"close clickable\">&times;</span>\n",
       "    </div>\n",
       "    <iframe class=\"fileview\" id=\"result6c98f502-body\"></iframe>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to track results use .show() or .logs() or in CLI: \n",
      "!mlrun get run 88288fb691df46f9874c0a86eedd1700 --project default , !mlrun logs 88288fb691df46f9874c0a86eedd1700 --project default\n",
      "> 2020-12-21 07:42:26,494 [info] run executed, status=completed\n"
     ]
    }
   ],
   "source": [
    "# run our functions with the relevant params\n",
    "run_res = fn.run(inputs={\"dataset\": \"iris.csv\"},\n",
    "                 artifact_path=artifact_path, watch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_res.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
