{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Job with Spark Operator\n",
    "Using spark operator for running spark job over k8s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2020-12-21 14:07:14,144 [info] starting remote build, image: .mlrun/func-default-sparkreadcsv-latest\n",
      "\u001b[36mINFO\u001b[0m[0020] Retrieving image manifest datanode-registry.iguazio-platform.app.hsbctesting3.iguazio-cd0.com:80/iguazio/spark-app:3.0_katyak_debug_b1089_20201214154653 \n",
      "\u001b[36mINFO\u001b[0m[0020] Retrieving image manifest datanode-registry.iguazio-platform.app.hsbctesting3.iguazio-cd0.com:80/iguazio/spark-app:3.0_katyak_debug_b1089_20201214154653 \n",
      "\u001b[36mINFO\u001b[0m[0020] Built cross stage deps: map[]                \n",
      "\u001b[36mINFO\u001b[0m[0020] Retrieving image manifest datanode-registry.iguazio-platform.app.hsbctesting3.iguazio-cd0.com:80/iguazio/spark-app:3.0_katyak_debug_b1089_20201214154653 \n",
      "\u001b[36mINFO\u001b[0m[0020] Retrieving image manifest datanode-registry.iguazio-platform.app.hsbctesting3.iguazio-cd0.com:80/iguazio/spark-app:3.0_katyak_debug_b1089_20201214154653 \n",
      "\u001b[36mINFO\u001b[0m[0020] Executing 0 build triggers                   \n",
      "\u001b[36mINFO\u001b[0m[0020] Unpacking rootfs as cmd RUN pip install matplotlib requires it. \n",
      "\u001b[36mINFO\u001b[0m[0059] RUN pip install matplotlib                   \n",
      "\u001b[36mINFO\u001b[0m[0059] Taking snapshot of full filesystem...        \n",
      "\u001b[36mINFO\u001b[0m[0064] cmd: /bin/sh                                 \n",
      "\u001b[36mINFO\u001b[0m[0064] args: [-c pip install matplotlib]            \n",
      "\u001b[36mINFO\u001b[0m[0064] util.Lookup returned: &{Uid:1000 Gid:1000 Username:iguazio Name: HomeDir:/igz} \n",
      "\u001b[36mINFO\u001b[0m[0064] performing slow lookup of group ids for iguazio \n",
      "\u001b[36mINFO\u001b[0m[0064] Running: [/bin/sh -c pip install matplotlib] \n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.3.3-cp37-cp37m-manylinux1_x86_64.whl (11.6 MB)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-8.0.1-cp37-cp37m-manylinux1_x86_64.whl (2.2 MB)\n",
      "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3\n",
      "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Collecting python-dateutil>=2.1\n",
      "  Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.3.1-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\n",
      "Collecting numpy>=1.15\n",
      "  Downloading numpy-1.19.4-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n",
      "Collecting six\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Installing collected packages: pillow, pyparsing, six, cycler, python-dateutil, kiwisolver, numpy, matplotlib\n",
      "  WARNING: The scripts f2py, f2py3 and f2py3.7 are installed in '/igz/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "Successfully installed cycler-0.10.0 kiwisolver-1.3.1 matplotlib-3.3.3 numpy-1.19.4 pillow-8.0.1 pyparsing-2.4.7 python-dateutil-2.8.1 six-1.15.0\n",
      "WARNING: You are using pip version 20.2.4; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
      "\u001b[36mINFO\u001b[0m[0070] Taking snapshot of full filesystem...        \n",
      "\u001b[36mINFO\u001b[0m[0074] RUN pip install mlrun                        \n",
      "\u001b[36mINFO\u001b[0m[0074] cmd: /bin/sh                                 \n",
      "\u001b[36mINFO\u001b[0m[0074] args: [-c pip install mlrun]                 \n",
      "\u001b[36mINFO\u001b[0m[0074] util.Lookup returned: &{Uid:1000 Gid:1000 Username:iguazio Name: HomeDir:/igz} \n",
      "\u001b[36mINFO\u001b[0m[0074] performing slow lookup of group ids for iguazio \n",
      "\u001b[36mINFO\u001b[0m[0074] Running: [/bin/sh -c pip install mlrun]      \n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting mlrun\n",
      "  Downloading mlrun-0.5.4-py3-none-any.whl (261 kB)\n",
      "Collecting nest-asyncio>=1.0.0\n",
      "  Downloading nest_asyncio-1.4.3-py3-none-any.whl (5.3 kB)\n",
      "Collecting docutils<0.16,>=0.13.1\n",
      "  Downloading docutils-0.15.2-py3-none-any.whl (547 kB)\n",
      "Collecting orjson<3.4,>=3\n",
      "  Downloading orjson-3.3.1-cp37-cp37m-manylinux2014_x86_64.whl (208 kB)\n",
      "Collecting boto3<1.16,>=1.9\n",
      "  Downloading boto3-1.15.18-py2.py3-none-any.whl (129 kB)\n",
      "Collecting pyyaml>=5.1.0\n",
      "  Downloading PyYAML-5.3.1.tar.gz (269 kB)\n",
      "Collecting pandas>=1.0.1\n",
      "  Downloading pandas-1.1.5-cp37-cp37m-manylinux1_x86_64.whl (9.5 MB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
      "Collecting tabulate<=0.8.3,>=0.8.0\n",
      "  Downloading tabulate-0.8.3.tar.gz (46 kB)\n",
      "Collecting v3io>=0.3.3\n",
      "  Downloading v3io-0.5.6-py3-none-any.whl (49 kB)\n",
      "Requirement already satisfied: matplotlib in /igz/.local/lib/python3.7/site-packages (from mlrun) (3.3.3)\n",
      "Collecting google-auth<2.0dev,>=1.19.1\n",
      "  Downloading google_auth-1.24.0-py2.py3-none-any.whl (114 kB)\n",
      "Collecting aiohttp<4.0.0dev,>=3.6.2\n",
      "  Downloading aiohttp-3.7.3-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
      "Collecting requests>=2.20.1\n",
      "  Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
      "Collecting sqlalchemy>=1.3.0\n",
      "  Downloading SQLAlchemy-1.3.22-cp37-cp37m-manylinux2010_x86_64.whl (1.3 MB)\n",
      "Collecting pydantic~=1.5\n",
      "  Downloading pydantic-1.7.3-cp37-cp37m-manylinux2014_x86_64.whl (9.1 MB)\n",
      "Collecting urllib3<1.25,>=1.24.2\n",
      "  Downloading urllib3-1.24.3-py2.py3-none-any.whl (118 kB)\n",
      "Collecting click==7.0\n",
      "  Downloading Click-7.0-py2.py3-none-any.whl (81 kB)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.11.1-py3-none-any.whl (285 kB)\n",
      "Collecting pyarrow>=0.13\n",
      "  Downloading pyarrow-2.0.0-cp37-cp37m-manylinux2014_x86_64.whl (17.7 MB)\n",
      "Collecting kfp==0.2.5\n",
      "  Downloading kfp-0.2.5.tar.gz (116 kB)\n",
      "Collecting GitPython>=2.1.0\n",
      "  Downloading GitPython-3.1.11-py3-none-any.whl (159 kB)\n",
      "Collecting azure-storage-blob\n",
      "  Downloading azure_storage_blob-12.6.0-py2.py3-none-any.whl (328 kB)\n",
      "Collecting ipython<7.17,>=5.5\n",
      "  Downloading ipython-7.16.1-py3-none-any.whl (785 kB)\n",
      "Collecting nuclio-jupyter>=0.8.7\n",
      "  Downloading nuclio_jupyter-0.8.9-py3-none-any.whl (47 kB)\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting botocore<1.19.0,>=1.18.18\n",
      "  Downloading botocore-1.18.18-py2.py3-none-any.whl (6.7 MB)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
      "Collecting pytz>=2017.2\n",
      "  Downloading pytz-2020.4-py2.py3-none-any.whl (509 kB)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /igz/.local/lib/python3.7/site-packages (from pandas>=1.0.1->mlrun) (1.19.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /igz/.local/lib/python3.7/site-packages (from pandas>=1.0.1->mlrun) (2.8.1)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-1.0.0-py3-none-any.whl (302 kB)\n",
      "Collecting scipy>=0.19.1\n",
      "  Downloading scipy-1.5.4-cp37-cp37m-manylinux1_x86_64.whl (25.9 MB)\n",
      "Collecting future>=0.18.2\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "Collecting ujson>=3.0.0\n",
      "  Downloading ujson-4.0.1-cp37-cp37m-manylinux1_x86_64.whl (179 kB)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /igz/.local/lib/python3.7/site-packages (from matplotlib->mlrun) (8.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /igz/.local/lib/python3.7/site-packages (from matplotlib->mlrun) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /igz/.local/lib/python3.7/site-packages (from matplotlib->mlrun) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /igz/.local/lib/python3.7/site-packages (from matplotlib->mlrun) (2.4.7)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.19.1->mlrun) (45.2.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /igz/.local/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.19.1->mlrun) (1.15.0)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading rsa-4.6-py3-none-any.whl (47 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n",
      "Collecting typing-extensions>=3.6.5\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting attrs>=17.3.0\n",
      "  Downloading attrs-20.3.0-py2.py3-none-any.whl (49 kB)\n",
      "Collecting chardet<4.0,>=2.0\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n",
      "Collecting async-timeout<4.0,>=3.0\n",
      "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2020.12.5-py2.py3-none-any.whl (147 kB)\n",
      "Collecting idna<3,>=2.5\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Collecting google-cloud-storage>=1.13.0\n",
      "  Downloading google_cloud_storage-1.35.0-py2.py3-none-any.whl (96 kB)\n",
      "Collecting kubernetes<=10.0.0,>=8.0.0\n",
      "  Downloading kubernetes-10.0.0-py2.py3-none-any.whl (1.5 MB)\n",
      "Collecting PyJWT>=1.6.4\n",
      "  Downloading PyJWT-1.7.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting cryptography>=2.4.2\n",
      "  Downloading cryptography-3.3.1-cp36-abi3-manylinux2010_x86_64.whl (2.6 MB)\n",
      "Collecting requests_toolbelt>=0.8.0\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "Collecting cloudpickle==1.1.1\n",
      "  Downloading cloudpickle-1.1.1-py2.py3-none-any.whl (17 kB)\n",
      "Collecting kfp-server-api<=0.1.40,>=0.1.18\n",
      "  Downloading kfp-server-api-0.1.40.tar.gz (38 kB)\n",
      "Collecting argo-models==2.2.1a\n",
      "  Downloading argo-models-2.2.1a0.tar.gz (28 kB)\n",
      "Collecting jsonschema>=3.0.1\n",
      "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "Collecting Deprecated\n",
      "  Downloading Deprecated-1.2.10-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting strip-hints\n",
      "  Downloading strip-hints-0.1.9.tar.gz (30 kB)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.5-py3-none-any.whl (63 kB)\n",
      "Collecting msrest>=0.6.10\n",
      "  Downloading msrest-0.6.19-py2.py3-none-any.whl (84 kB)\n",
      "Collecting azure-core<2.0.0,>=1.9.0\n",
      "  Downloading azure_core-1.9.0-py2.py3-none-any.whl (124 kB)\n",
      "Collecting jedi>=0.10\n",
      "  Downloading jedi-0.17.2-py2.py3-none-any.whl (1.4 MB)\n",
      "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
      "  Downloading prompt_toolkit-3.0.8-py3-none-any.whl (355 kB)\n",
      "Collecting decorator\n",
      "  Downloading decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
      "Collecting pexpect; sys_platform != \"win32\"\n",
      "  Downloading pexpect-4.8.0-py2.py3-none-any.whl (59 kB)\n",
      "Collecting pygments\n",
      "  Downloading Pygments-2.7.3-py3-none-any.whl (950 kB)\n",
      "Collecting pickleshare\n",
      "  Downloading pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB)\n",
      "Collecting backcall\n",
      "  Downloading backcall-0.2.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting traitlets>=4.2\n",
      "  Downloading traitlets-5.0.5-py3-none-any.whl (100 kB)\n",
      "Collecting notebook>=5.2.0\n",
      "  Downloading notebook-6.1.5-py3-none-any.whl (9.5 MB)\n",
      "Collecting nbconvert>=5.4\n",
      "  Downloading nbconvert-6.0.7-py3-none-any.whl (552 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting google-resumable-media<2.0dev,>=1.2.0\n",
      "  Downloading google_resumable_media-1.2.0-py2.py3-none-any.whl (75 kB)\n",
      "Collecting google-cloud-core<2.0dev,>=1.4.1\n",
      "  Downloading google_cloud_core-1.5.0-py2.py3-none-any.whl (27 kB)\n",
      "Collecting requests-oauthlib\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0\n",
      "  Downloading websocket_client-0.57.0-py2.py3-none-any.whl (200 kB)\n",
      "Collecting cffi>=1.12\n",
      "  Downloading cffi-1.14.4-cp37-cp37m-manylinux1_x86_64.whl (402 kB)\n",
      "Collecting importlib-metadata; python_version < \"3.8\"\n",
      "  Downloading importlib_metadata-3.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting pyrsistent>=0.14.0\n",
      "  Downloading pyrsistent-0.17.3.tar.gz (106 kB)\n",
      "Collecting wrapt<2,>=1.10\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.7/site-packages (from strip-hints->kfp==0.2.5->mlrun) (0.34.2)\n",
      "Collecting smmap<4,>=3.0.1\n",
      "  Downloading smmap-3.0.4-py2.py3-none-any.whl (25 kB)\n",
      "Collecting isodate>=0.6.0\n",
      "  Downloading isodate-0.6.0-py2.py3-none-any.whl (45 kB)\n",
      "Collecting parso<0.8.0,>=0.7.0\n",
      "  Downloading parso-0.7.1-py2.py3-none-any.whl (109 kB)\n",
      "Collecting wcwidth\n",
      "  Downloading wcwidth-0.2.5-py2.py3-none-any.whl (30 kB)\n",
      "Collecting ptyprocess>=0.5\n",
      "  Downloading ptyprocess-0.6.0-py2.py3-none-any.whl (39 kB)\n",
      "Collecting ipython-genutils\n",
      "  Downloading ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting nbformat\n",
      "  Downloading nbformat-5.0.8-py3-none-any.whl (172 kB)\n",
      "Collecting terminado>=0.8.3\n",
      "  Downloading terminado-0.9.1-py3-none-any.whl (13 kB)\n",
      "Collecting jupyter-client>=5.3.4\n",
      "  Downloading jupyter_client-6.1.7-py3-none-any.whl (108 kB)\n",
      "Collecting prometheus-client\n",
      "  Downloading prometheus_client-0.9.0-py2.py3-none-any.whl (53 kB)\n",
      "Collecting ipykernel\n",
      "  Downloading ipykernel-5.4.2-py3-none-any.whl (119 kB)\n",
      "Collecting Send2Trash\n",
      "  Downloading Send2Trash-1.5.0-py3-none-any.whl (12 kB)\n",
      "Collecting jupyter-core>=4.6.1\n",
      "  Downloading jupyter_core-4.7.0-py3-none-any.whl (82 kB)\n",
      "Collecting tornado>=5.0\n",
      "  Downloading tornado-6.1-cp37-cp37m-manylinux2010_x86_64.whl (428 kB)\n",
      "Collecting argon2-cffi\n",
      "  Downloading argon2_cffi-20.1.0-cp35-abi3-manylinux1_x86_64.whl (97 kB)\n",
      "Collecting pyzmq>=17\n",
      "  Downloading pyzmq-20.0.0-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\n",
      "Collecting jinja2\n",
      "  Downloading Jinja2-2.11.2-py2.py3-none-any.whl (125 kB)\n",
      "Collecting mistune<2,>=0.8.1\n",
      "  Downloading mistune-0.8.4-py2.py3-none-any.whl (16 kB)\n",
      "Collecting nbclient<0.6.0,>=0.5.0\n",
      "  Downloading nbclient-0.5.1-py3-none-any.whl (65 kB)\n",
      "Collecting jupyterlab-pygments\n",
      "  Downloading jupyterlab_pygments-0.1.2-py2.py3-none-any.whl (4.6 kB)\n",
      "Collecting entrypoints>=0.2.2\n",
      "  Downloading entrypoints-0.3-py2.py3-none-any.whl (11 kB)\n",
      "Collecting defusedxml\n",
      "  Downloading defusedxml-0.6.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting testpath\n",
      "  Downloading testpath-0.4.4-py2.py3-none-any.whl (163 kB)\n",
      "Collecting pandocfilters>=1.4.1\n",
      "  Downloading pandocfilters-1.4.3.tar.gz (16 kB)\n",
      "Collecting bleach\n",
      "  Downloading bleach-3.2.1-py2.py3-none-any.whl (145 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0; python_version >= \"3.5\"\n",
      "  Downloading google_crc32c-1.1.0-cp37-cp37m-manylinux2010_x86_64.whl (39 kB)\n",
      "Collecting google-api-core<2.0.0dev,>=1.21.0\n",
      "  Downloading google_api_core-1.24.1-py2.py3-none-any.whl (92 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Collecting pycparser\n",
      "  Downloading pycparser-2.20-py2.py3-none-any.whl (112 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.4.0-py3-none-any.whl (5.2 kB)\n",
      "Collecting MarkupSafe>=0.23\n",
      "  Downloading MarkupSafe-1.1.1-cp37-cp37m-manylinux1_x86_64.whl (27 kB)\n",
      "Collecting async-generator\n",
      "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
      "Collecting packaging\n",
      "  Downloading packaging-20.8-py2.py3-none-any.whl (39 kB)\n",
      "Collecting webencodings\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.6.0\n",
      "  Downloading googleapis_common_protos-1.52.0-py2.py3-none-any.whl (100 kB)\n",
      "Collecting protobuf>=3.12.0\n",
      "  Downloading protobuf-3.14.0-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n",
      "Building wheels for collected packages: pyyaml, tabulate, kfp, future, kfp-server-api, argo-models, strip-hints, pyrsistent, wrapt, pandocfilters\n",
      "  Building wheel for pyyaml (setup.py): started\n",
      "  Building wheel for pyyaml (setup.py): finished with status 'done'\n",
      "  Created wheel for pyyaml: filename=PyYAML-5.3.1-cp37-cp37m-linux_x86_64.whl size=44619 sha256=e4aebfda3377028365a9e6fbaba7696d98e2fccd40f50cb9f1168e35e92729b4\n",
      "  Stored in directory: /igz/.cache/pip/wheels/5e/03/1e/e1e954795d6f35dfc7b637fe2277bff021303bd9570ecea653\n",
      "  Building wheel for tabulate (setup.py): started\n",
      "  Building wheel for tabulate (setup.py): finished with status 'done'\n",
      "  Created wheel for tabulate: filename=tabulate-0.8.3-py3-none-any.whl size=23378 sha256=8d2fd9634dcaf5c90589dc1ce8a1207d894a2df3d335c92a7dba24b9729a2b8f\n",
      "  Stored in directory: /igz/.cache/pip/wheels/b8/a2/a6/812a8a9735b090913e109133c7c20aaca4cf07e8e18837714f\n",
      "  Building wheel for kfp (setup.py): started\n",
      "  Building wheel for kfp (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp: filename=kfp-0.2.5-py3-none-any.whl size=159978 sha256=2bdedb6493a423098138d2e789f0018a890941f6f3f2b19c59aa8faec9456e49\n",
      "  Stored in directory: /igz/.cache/pip/wheels/98/74/7e/0a882d654bdf82d039460ab5c6adf8724ae56e277de7c0eaea\n",
      "  Building wheel for future (setup.py): started\n",
      "  Building wheel for future (setup.py): finished with status 'done'\n",
      "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491058 sha256=3437faa44b4de30ba3c2e19094d29813d042bda2ff022913efbc632f54215f42\n",
      "  Stored in directory: /igz/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
      "  Building wheel for kfp-server-api (setup.py): started\n",
      "  Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-0.1.40-py3-none-any.whl size=102468 sha256=773520690c83aaa23f7116418fbed8254d9e772ac35077899fd769ee43cfdc5e\n",
      "  Stored in directory: /igz/.cache/pip/wheels/01/e3/43/3972dea76ee89e35f090b313817089043f2609236cf560069d\n",
      "  Building wheel for argo-models (setup.py): started\n",
      "  Building wheel for argo-models (setup.py): finished with status 'done'\n",
      "  Created wheel for argo-models: filename=argo_models-2.2.1a0-py3-none-any.whl size=57307 sha256=36f0b2dc1847e0c2194dc824bd50898c068369e7d164ca196ff6015ca47ec597\n",
      "  Stored in directory: /igz/.cache/pip/wheels/a9/4b/fd/cdd013bd2ad1a7162ecfaf954e9f1bb605174a20e3c02016b7\n",
      "  Building wheel for strip-hints (setup.py): started\n",
      "  Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "  Created wheel for strip-hints: filename=strip_hints-0.1.9-py2.py3-none-any.whl size=20993 sha256=04ce6607b1f40c773286161a81fc17ca5a374aef5a2a580e63327934081e3c15\n",
      "  Stored in directory: /igz/.cache/pip/wheels/2d/b8/4e/a3ec111d2db63cec88121bd7c0ab1a123bce3b55dd19dda5c1\n",
      "  Building wheel for pyrsistent (setup.py): started\n",
      "  Building wheel for pyrsistent (setup.py): finished with status 'done'\n",
      "  Created wheel for pyrsistent: filename=pyrsistent-0.17.3-cp37-cp37m-linux_x86_64.whl size=55874 sha256=064b52025c3d40c93843bd40710476304d784db0ddec4cf43287efca870f599b\n",
      "  Stored in directory: /igz/.cache/pip/wheels/a5/52/bf/71258a1d7b3c8cbe1ee53f9314c6f65f20385481eaee573cc5\n",
      "  Building wheel for wrapt (setup.py): started\n",
      "  Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "  Created wheel for wrapt: filename=wrapt-1.12.1-py3-none-any.whl size=19553 sha256=5adc17ef5a33d988ea1f13c5c5f2befd13236403a174c887081e7b283debe45c\n",
      "  Stored in directory: /igz/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
      "  Building wheel for pandocfilters (setup.py): started\n",
      "  Building wheel for pandocfilters (setup.py): finished with status 'done'\n",
      "  Created wheel for pandocfilters: filename=pandocfilters-1.4.3-py3-none-any.whl size=7991 sha256=7b60f02f42af43f031bc810b0c52628f0ad324c63372d64c5262a03ba8e1fed7\n",
      "  Stored in directory: /igz/.cache/pip/wheels/42/81/34/545dc2fbf0e9137811e901108d37fc04650e81d48f97078000\n",
      "Successfully built pyyaml tabulate kfp future kfp-server-api argo-models strip-hints pyrsistent wrapt pandocfilters\n",
      "Installing collected packages: nest-asyncio, docutils, orjson, jmespath, urllib3, botocore, s3transfer, boto3, pyyaml, pytz, pandas, threadpoolctl, joblib, scipy, scikit-learn, tabulate, certifi, idna, chardet, requests, future, ujson, v3io, cachetools, pyasn1, pyasn1-modules, rsa, google-auth, multidict, typing-extensions, attrs, yarl, async-timeout, aiohttp, sqlalchemy, pydantic, click, seaborn, pyarrow, pycparser, cffi, google-crc32c, google-resumable-media, protobuf, googleapis-common-protos, google-api-core, google-cloud-core, google-cloud-storage, oauthlib, requests-oauthlib, websocket-client, kubernetes, PyJWT, cryptography, requests-toolbelt, cloudpickle, kfp-server-api, argo-models, zipp, importlib-metadata, pyrsistent, jsonschema, wrapt, Deprecated, strip-hints, kfp, smmap, gitdb, GitPython, isodate, msrest, azure-core, azure-storage-blob, parso, jedi, wcwidth, prompt-toolkit, decorator, ptyprocess, pexpect, pygments, pickleshare, backcall, ipython-genutils, traitlets, ipython, jupyter-core, nbformat, tornado, terminado, pyzmq, jupyter-client, prometheus-client, ipykernel, mistune, MarkupSafe, jinja2, async-generator, nbclient, jupyterlab-pygments, entrypoints, defusedxml, testpath, pandocfilters, packaging, webencodings, bleach, nbconvert, Send2Trash, argon2-cffi, notebook, nuclio-jupyter, mlrun\n",
      "  WARNING: The script tabulate is installed in '/igz/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script chardetect is installed in '/igz/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts futurize and pasteurize are installed in '/igz/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts pyrsa-decrypt, pyrsa-encrypt, pyrsa-keygen, pyrsa-priv2pub, pyrsa-sign and pyrsa-verify are installed in '/igz/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script plasma_store is installed in '/igz/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script pyjwt is installed in '/igz/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script jsonschema is installed in '/igz/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script strip-hints is installed in '/igz/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts dsl-compile and kfp are installed in '/igz/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script pygmentize is installed in '/igz/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts iptest, iptest3, ipython and ipython3 are installed in '/igz/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts jupyter, jupyter-migrate and jupyter-troubleshoot are installed in '/igz/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script jupyter-trust is installed in '/igz/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts jupyter-kernel, jupyter-kernelspec and jupyter-run are installed in '/igz/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script jupyter-nbconvert is installed in '/igz/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts jupyter-bundlerextension, jupyter-nbextension, jupyter-notebook and jupyter-serverextension are installed in '/igz/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script nuclio is installed in '/igz/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script mlrun is installed in '/igz/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "Successfully installed Deprecated-1.2.10 GitPython-3.1.11 MarkupSafe-1.1.1 PyJWT-1.7.1 Send2Trash-1.5.0 aiohttp-3.7.3 argo-models-2.2.1a0 argon2-cffi-20.1.0 async-generator-1.10 async-timeout-3.0.1 attrs-20.3.0 azure-core-1.9.0 azure-storage-blob-12.6.0 backcall-0.2.0 bleach-3.2.1 boto3-1.15.18 botocore-1.18.18 cachetools-4.2.0 certifi-2020.12.5 cffi-1.14.4 chardet-3.0.4 click-7.0 cloudpickle-1.1.1 cryptography-3.3.1 decorator-4.4.2 defusedxml-0.6.0 docutils-0.15.2 entrypoints-0.3 future-0.18.2 gitdb-4.0.5 google-api-core-1.24.1 google-auth-1.24.0 google-cloud-core-1.5.0 google-cloud-storage-1.35.0 google-crc32c-1.1.0 google-resumable-media-1.2.0 googleapis-common-protos-1.52.0 idna-2.10 importlib-metadata-3.3.0 ipykernel-5.4.2 ipython-7.16.1 ipython-genutils-0.2.0 isodate-0.6.0 jedi-0.17.2 jinja2-2.11.2 jmespath-0.10.0 joblib-1.0.0 jsonschema-3.2.0 jupyter-client-6.1.7 jupyter-core-4.7.0 jupyterlab-pygments-0.1.2 kfp-0.2.5 kfp-server-api-0.1.40 kubernetes-10.0.0 mistune-0.8.4 mlrun-0.5.4 msrest-0.6.19 multidict-5.1.0 nbclient-0.5.1 nbconvert-6.0.7 nbformat-5.0.8 nest-asyncio-1.4.3 notebook-6.1.5 nuclio-jupyter-0.8.9 oauthlib-3.1.0 orjson-3.3.1 packaging-20.8 pandas-1.1.5 pandocfilters-1.4.3 parso-0.7.1 pexpect-4.8.0 pickleshare-0.7.5 prometheus-client-0.9.0 prompt-toolkit-3.0.8 protobuf-3.14.0 ptyprocess-0.6.0 pyarrow-2.0.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pydantic-1.7.3 pygments-2.7.3 pyrsistent-0.17.3 pytz-2020.4 pyyaml-5.3.1 pyzmq-20.0.0 requests-2.25.1 requests-oauthlib-1.3.0 requests-toolbelt-0.9.1 rsa-4.6 s3transfer-0.3.3 scikit-learn-0.23.2 scipy-1.5.4 seaborn-0.11.1 smmap-3.0.4 sqlalchemy-1.3.22 strip-hints-0.1.9 tabulate-0.8.3 terminado-0.9.1 testpath-0.4.4 threadpoolctl-2.1.0 tornado-6.1 traitlets-5.0.5 typing-extensions-3.7.4.3 ujson-4.0.1 urllib3-1.24.3 v3io-0.5.6 wcwidth-0.2.5 webencodings-0.5.1 websocket-client-0.57.0 wrapt-1.12.1 yarl-1.6.3 zipp-3.4.0\n",
      "WARNING: You are using pip version 20.2.4; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
      "\u001b[36mINFO\u001b[0m[0114] Taking snapshot of full filesystem...        \n",
      "> 2020-12-21 14:09:56,568 [info] starting run sparkreadcsv uid=ddb40aabe2ca48a78b9b09437c4b8ef6 DB=http://mlrun-api:8080\n",
      "> 2020-12-21 14:10:28,925 [info] UI is available while the job is running: http://sparkreadcsv-d00ee42e.default-tenant.app.hsbctesting3.iguazio-cd0.com\n",
      "++ id -u\n",
      "+ myuid=1000\n",
      "++ id -g\n",
      "+ mygid=1000\n",
      "+ set +e\n",
      "++ getent passwd 1000\n",
      "+ uidentry=iguazio:x:1000:1000::/igz:/bin/bash\n",
      "+ set -e\n",
      "+ '[' -z iguazio:x:1000:1000::/igz:/bin/bash ']'\n",
      "+ SPARK_K8S_CMD=driver-py\n",
      "+ case \"$SPARK_K8S_CMD\" in\n",
      "+ shift 1\n",
      "+ SPARK_CLASSPATH=':/spark/jars/*'\n",
      "+ env\n",
      "+ grep SPARK_JAVA_OPT_\n",
      "+ sort -t_ -k4 -n\n",
      "+ sed 's/[^=]*=\\(.*\\)/\\1/g'\n",
      "+ readarray -t SPARK_EXECUTOR_JAVA_OPTS\n",
      "+ '[' -n '' ']'\n",
      "+ '[' -n file:///igz/java/libs/v3io-pyspark.zip ']'\n",
      "+ PYTHONPATH=:file:///igz/java/libs/v3io-pyspark.zip\n",
      "+ PYSPARK_ARGS=\n",
      "+ '[' -n '-spark.eventLog.enabled true' ']'\n",
      "+ PYSPARK_ARGS='-spark.eventLog.enabled true'\n",
      "+ R_ARGS=\n",
      "+ '[' -n '' ']'\n",
      "+ '[' 2 == 2 ']'\n",
      "++ python -V\n",
      "+ pyv='Python 3.7.6'\n",
      "+ export PYTHON_VERSION=3.7.6\n",
      "+ PYTHON_VERSION=3.7.6\n",
      "+ export PYSPARK_PYTHON=python\n",
      "+ PYSPARK_PYTHON=python\n",
      "+ export PYSPARK_DRIVER_PYTHON=python\n",
      "+ PYSPARK_DRIVER_PYTHON=python\n",
      "+ case \"$SPARK_K8S_CMD\" in\n",
      "+ CMD=(\"$SPARK_HOME/bin/spark-submit\" --conf \"spark.driver.bindAddress=$SPARK_DRIVER_BIND_ADDRESS\" --deploy-mode client \"$@\" $PYSPARK_PRIMARY $PYSPARK_ARGS)\n",
      "+ exec /usr/bin/tini -s -- /spark/bin/spark-submit --conf spark.driver.bindAddress=10.200.0.56 --deploy-mode client --properties-file /opt/spark/conf/spark.properties --class org.apache.spark.deploy.PythonRunner /User/sparkreadCSV.py -spark.eventLog.enabled true\n",
      "\n",
      "20/12/21 14:10:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "> 2020-12-21 14:10:33,513 [info] logging run results to: http://mlrun-api:8080\n",
      "> 2020-12-21 14:10:33,522 [warning] warning!, server (0.6.0-rc6) and client (0.5.4) ver dont match\n",
      "20/12/21 14:10:33 INFO spark.SparkContext: Running Spark version 2.4.5\n",
      "20/12/21 14:10:33 INFO spark.SparkContext: Submitted application: Spark job\n",
      "20/12/21 14:10:34 INFO spark.SecurityManager: Changing view acls to: iguazio\n",
      "20/12/21 14:10:34 INFO spark.SecurityManager: Changing modify acls to: iguazio\n",
      "20/12/21 14:10:34 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "20/12/21 14:10:34 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "20/12/21 14:10:34 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(iguazio); groups with view permissions: Set(); users  with modify permissions: Set(iguazio); groups with modify permissions: Set()\n",
      "20/12/21 14:10:34 INFO util.Utils: Successfully started service 'sparkDriver' on port 7078.\n",
      "20/12/21 14:10:34 INFO spark.SparkEnv: Registering MapOutputTracker\n",
      "20/12/21 14:10:34 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
      "20/12/21 14:10:34 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "20/12/21 14:10:34 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "20/12/21 14:10:34 INFO storage.DiskBlockManager: Created local directory at /var/data/spark-43365362-bc16-4a11-ab39-f4a5f3d188c9/blockmgr-a6ca2169-c087-46ad-9365-bedd514d4f75\n",
      "20/12/21 14:10:34 INFO memory.MemoryStore: MemoryStore started with capacity 93.3 MB\n",
      "20/12/21 14:10:34 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "20/12/21 14:10:34 INFO util.log: Logging initialized @5935ms\n",
      "20/12/21 14:10:34 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\n",
      "20/12/21 14:10:34 INFO server.Server: Started @6100ms\n",
      "20/12/21 14:10:34 INFO server.AbstractConnector: Started ServerConnector@400ba125{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "20/12/21 14:10:34 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "20/12/21 14:10:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3b8c7245{/jobs,null,AVAILABLE,@Spark}\n",
      "20/12/21 14:10:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5d8e35f{/jobs/json,null,AVAILABLE,@Spark}\n",
      "20/12/21 14:10:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3728290{/jobs/job,null,AVAILABLE,@Spark}\n",
      "20/12/21 14:10:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25526bc9{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "20/12/21 14:10:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6752a627{/stages,null,AVAILABLE,@Spark}\n",
      "20/12/21 14:10:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@70a3ce5f{/stages/json,null,AVAILABLE,@Spark}\n",
      "20/12/21 14:10:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@250acb2a{/stages/stage,null,AVAILABLE,@Spark}\n",
      "20/12/21 14:10:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@f9ef4e3{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "20/12/21 14:10:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7c3e2da1{/stages/pool,null,AVAILABLE,@Spark}\n",
      "20/12/21 14:10:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@689969bf{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "20/12/21 14:10:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1acd7fd{/storage,null,AVAILABLE,@Spark}\n",
      "20/12/21 14:10:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@79ed02c7{/storage/json,null,AVAILABLE,@Spark}\n",
      "20/12/21 14:10:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@74337b7e{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "20/12/21 14:10:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@574420d6{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "20/12/21 14:10:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@303006fc{/environment,null,AVAILABLE,@Spark}\n",
      "20/12/21 14:10:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@21dac107{/environment/json,null,AVAILABLE,@Spark}\n",
      "20/12/21 14:10:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4e5da6c9{/executors,null,AVAILABLE,@Spark}\n",
      "20/12/21 14:10:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@38697d44{/executors/json,null,AVAILABLE,@Spark}\n",
      "20/12/21 14:10:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@8de9b53{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "20/12/21 14:10:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7ffba1b6{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "20/12/21 14:10:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5c332b6e{/static,null,AVAILABLE,@Spark}\n",
      "20/12/21 14:10:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7cdc59cd{/,null,AVAILABLE,@Spark}\n",
      "20/12/21 14:10:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3eea8ac5{/api,null,AVAILABLE,@Spark}\n",
      "20/12/21 14:10:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4e268fa{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "20/12/21 14:10:34 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ac28d69{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "20/12/21 14:10:34 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://sparkreadcsv-d00ee42e-1608559809091-driver-svc.default-tenant.svc:4040\n",
      "20/12/21 14:10:34 INFO spark.SparkContext: Added JAR file:///spark/v3io-libs/v3io-hcfs_2.11.jar at spark://sparkreadcsv-d00ee42e-1608559809091-driver-svc.default-tenant.svc:7078/jars/v3io-hcfs_2.11.jar with timestamp 1608559834883\n",
      "20/12/21 14:10:34 INFO spark.SparkContext: Added JAR file:///spark/v3io-libs/v3io-spark2-streaming_2.11.jar at spark://sparkreadcsv-d00ee42e-1608559809091-driver-svc.default-tenant.svc:7078/jars/v3io-spark2-streaming_2.11.jar with timestamp 1608559834883\n",
      "20/12/21 14:10:34 INFO spark.SparkContext: Added JAR file:///spark/v3io-libs/v3io-spark2-object-dataframe_2.11.jar at spark://sparkreadcsv-d00ee42e-1608559809091-driver-svc.default-tenant.svc:7078/jars/v3io-spark2-object-dataframe_2.11.jar with timestamp 1608559834884\n",
      "20/12/21 14:10:34 INFO spark.SparkContext: Added JAR file:///igz/java/libs/scala-library-2.11.12.jar at spark://sparkreadcsv-d00ee42e-1608559809091-driver-svc.default-tenant.svc:7078/jars/scala-library-2.11.12.jar with timestamp 1608559834884\n",
      "20/12/21 14:10:34 INFO spark.SparkContext: Added file file:///igz/java/libs/v3io-pyspark.zip at spark://sparkreadcsv-d00ee42e-1608559809091-driver-svc.default-tenant.svc:7078/files/v3io-pyspark.zip with timestamp 1608559834989\n",
      "20/12/21 14:10:34 INFO util.Utils: Copying /igz/java/libs/v3io-pyspark.zip to /var/data/spark-43365362-bc16-4a11-ab39-f4a5f3d188c9/spark-9ecdcfd7-3576-4574-b50f-1881bbe4a577/userFiles-bbdd7273-635b-408f-be94-74df23418618/v3io-pyspark.zip\n",
      "20/12/21 14:10:35 INFO spark.SparkContext: Added file file:///User/sparkreadCSV.py at spark://sparkreadcsv-d00ee42e-1608559809091-driver-svc.default-tenant.svc:7078/files/sparkreadCSV.py with timestamp 1608559835082\n",
      "20/12/21 14:10:35 INFO util.Utils: Copying /User/sparkreadCSV.py to /var/data/spark-43365362-bc16-4a11-ab39-f4a5f3d188c9/spark-9ecdcfd7-3576-4574-b50f-1881bbe4a577/userFiles-bbdd7273-635b-408f-be94-74df23418618/sparkreadCSV.py\n",
      "20/12/21 14:10:35 WARN spark.SparkContext: The path file:///igz/java/libs/v3io-pyspark.zip has been added already. Overwriting of added paths is not supported in the current version.\n",
      "20/12/21 14:10:35 WARN spark.SparkContext: The path file:///igz/java/libs/v3io-pyspark.zip has been added already. Overwriting of added paths is not supported in the current version.\n",
      "20/12/21 14:10:37 INFO k8s.ExecutorPodsAllocator: Going to request 2 executors from Kubernetes.\n",
      "20/12/21 14:10:37 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 7079.\n",
      "20/12/21 14:10:37 INFO netty.NettyBlockTransferService: Server created on sparkreadcsv-d00ee42e-1608559809091-driver-svc.default-tenant.svc:7079\n",
      "20/12/21 14:10:37 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "20/12/21 14:10:37 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, sparkreadcsv-d00ee42e-1608559809091-driver-svc.default-tenant.svc, 7079, None)\n",
      "20/12/21 14:10:37 INFO storage.BlockManagerMasterEndpoint: Registering block manager sparkreadcsv-d00ee42e-1608559809091-driver-svc.default-tenant.svc:7079 with 93.3 MB RAM, BlockManagerId(driver, sparkreadcsv-d00ee42e-1608559809091-driver-svc.default-tenant.svc, 7079, None)\n",
      "20/12/21 14:10:37 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, sparkreadcsv-d00ee42e-1608559809091-driver-svc.default-tenant.svc, 7079, None)\n",
      "20/12/21 14:10:37 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, sparkreadcsv-d00ee42e-1608559809091-driver-svc.default-tenant.svc, 7079, None)\n",
      "20/12/21 14:10:37 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7048d4ad{/metrics/json,null,AVAILABLE,@Spark}\n",
      "20/12/21 14:10:43 INFO k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.200.0.58:51108) with ID 1\n",
      "20/12/21 14:10:43 INFO k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.200.0.57:58324) with ID 2\n",
      "20/12/21 14:10:44 INFO k8s.KubernetesClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\n",
      "20/12/21 14:10:44 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/spark/spark-warehouse').\n",
      "20/12/21 14:10:44 INFO internal.SharedState: Warehouse path is 'file:/spark/spark-warehouse'.\n",
      "20/12/21 14:10:44 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2abe4715{/SQL,null,AVAILABLE,@Spark}\n",
      "20/12/21 14:10:44 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6ba5e117{/SQL/json,null,AVAILABLE,@Spark}\n",
      "20/12/21 14:10:44 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1683d47e{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "20/12/21 14:10:44 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@67fb053b{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "20/12/21 14:10:44 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3b35e490{/static/sql,null,AVAILABLE,@Spark}\n",
      "20/12/21 14:10:44 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.200.0.58:35528 with 114.6 MB RAM, BlockManagerId(1, 10.200.0.58, 35528, None)\n",
      "20/12/21 14:10:44 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.200.0.57:45700 with 114.6 MB RAM, BlockManagerId(2, 10.200.0.57, 45700, None)\n",
      "20/12/21 14:10:44 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\n",
      "20/12/21 14:10:45 INFO slf_4j.Slf4jLogger: Slf4jLogger started\n",
      "20/12/21 14:10:47 INFO datasources.InMemoryFileIndex: It took 242 ms to list leaf files for 1 paths.\n",
      "20/12/21 14:10:47 INFO datasources.InMemoryFileIndex: It took 112 ms to list leaf files for 2 paths.\n",
      "20/12/21 14:10:50 INFO datasources.FileSourceStrategy: Pruning directories with: \n",
      "20/12/21 14:10:50 INFO datasources.FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)\n",
      "20/12/21 14:10:50 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "20/12/21 14:10:50 INFO execution.FileSourceScanExec: Pushed Filters: \n",
      "20/12/21 14:10:50 INFO codegen.CodeGenerator: Code generated in 326.911197 ms\n",
      "20/12/21 14:10:51 INFO codegen.CodeGenerator: Code generated in 14.609531 ms\n",
      "20/12/21 14:10:51 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 306.6 KB, free 93.0 MB)\n",
      "20/12/21 14:10:51 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.0 KB, free 93.0 MB)\n",
      "20/12/21 14:10:51 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on sparkreadcsv-d00ee42e-1608559809091-driver-svc.default-tenant.svc:7079 (size: 26.0 KB, free: 93.3 MB)\n",
      "20/12/21 14:10:51 INFO spark.SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0\n",
      "20/12/21 14:10:51 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4197053 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "20/12/21 14:10:51 INFO spark.SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0\n",
      "20/12/21 14:10:51 INFO scheduler.DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "20/12/21 14:10:51 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)\n",
      "20/12/21 14:10:51 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "20/12/21 14:10:51 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "20/12/21 14:10:51 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at load at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "20/12/21 14:10:51 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.9 KB, free 93.0 MB)\n",
      "20/12/21 14:10:51 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KB, free 93.0 MB)\n",
      "20/12/21 14:10:51 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on sparkreadcsv-d00ee42e-1608559809091-driver-svc.default-tenant.svc:7079 (size: 4.6 KB, free: 93.3 MB)\n",
      "20/12/21 14:10:51 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1163\n",
      "20/12/21 14:10:51 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "20/12/21 14:10:51 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks\n",
      "20/12/21 14:10:51 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 10.200.0.58, executor 1, partition 0, PROCESS_LOCAL, 8259 bytes)\n",
      "20/12/21 14:10:52 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.200.0.58:35528 (size: 4.6 KB, free: 114.6 MB)\n",
      "20/12/21 14:10:53 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.200.0.58:35528 (size: 26.0 KB, free: 114.6 MB)\n",
      "20/12/21 14:10:56 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 4684 ms on 10.200.0.58 (executor 1) (1/1)\n",
      "20/12/21 14:10:56 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "20/12/21 14:10:56 INFO scheduler.DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 4.792 s\n",
      "20/12/21 14:10:56 INFO scheduler.DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 4.830441 s\n",
      "20/12/21 14:10:56 INFO spark.ContextCleaner: Cleaned accumulator 1\n",
      "20/12/21 14:10:56 INFO spark.ContextCleaner: Cleaned accumulator 9\n",
      "20/12/21 14:10:56 INFO spark.ContextCleaner: Cleaned accumulator 7\n",
      "20/12/21 14:10:56 INFO spark.ContextCleaner: Cleaned accumulator 14\n",
      "20/12/21 14:10:56 INFO spark.ContextCleaner: Cleaned accumulator 17\n",
      "20/12/21 14:10:56 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on sparkreadcsv-d00ee42e-1608559809091-driver-svc.default-tenant.svc:7079 in memory (size: 26.0 KB, free: 93.3 MB)\n",
      "20/12/21 14:10:56 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 10.200.0.58:35528 in memory (size: 26.0 KB, free: 114.6 MB)\n",
      "20/12/21 14:10:56 INFO spark.ContextCleaner: Cleaned accumulator 19\n",
      "20/12/21 14:10:56 INFO spark.ContextCleaner: Cleaned accumulator 20\n",
      "20/12/21 14:10:56 INFO spark.ContextCleaner: Cleaned accumulator 18\n",
      "20/12/21 14:10:56 INFO spark.ContextCleaner: Cleaned accumulator 15\n",
      "20/12/21 14:10:56 INFO spark.ContextCleaner: Cleaned accumulator 27\n",
      "20/12/21 14:10:56 INFO spark.ContextCleaner: Cleaned accumulator 4\n",
      "20/12/21 14:10:56 INFO spark.ContextCleaner: Cleaned accumulator 22\n",
      "20/12/21 14:10:56 INFO spark.ContextCleaner: Cleaned accumulator 2\n",
      "20/12/21 14:10:56 INFO spark.ContextCleaner: Cleaned accumulator 11\n",
      "20/12/21 14:10:56 INFO spark.ContextCleaner: Cleaned accumulator 3\n",
      "20/12/21 14:10:56 INFO spark.ContextCleaner: Cleaned accumulator 25\n",
      "20/12/21 14:10:56 INFO spark.ContextCleaner: Cleaned accumulator 13\n",
      "20/12/21 14:10:56 INFO spark.ContextCleaner: Cleaned accumulator 28\n",
      "20/12/21 14:10:56 INFO spark.ContextCleaner: Cleaned accumulator 29\n",
      "20/12/21 14:10:56 INFO spark.ContextCleaner: Cleaned accumulator 31\n",
      "20/12/21 14:10:56 INFO spark.ContextCleaner: Cleaned accumulator 8\n",
      "20/12/21 14:10:56 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on sparkreadcsv-d00ee42e-1608559809091-driver-svc.default-tenant.svc:7079 in memory (size: 4.6 KB, free: 93.3 MB)\n",
      "20/12/21 14:10:56 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.200.0.58:35528 in memory (size: 4.6 KB, free: 114.6 MB)\n",
      "20/12/21 14:10:56 INFO spark.ContextCleaner: Cleaned accumulator 30\n",
      "20/12/21 14:10:56 INFO spark.ContextCleaner: Cleaned accumulator 10\n",
      "20/12/21 14:10:56 INFO spark.ContextCleaner: Cleaned accumulator 6\n",
      "20/12/21 14:10:56 INFO spark.ContextCleaner: Cleaned accumulator 16\n",
      "20/12/21 14:10:56 INFO spark.ContextCleaner: Cleaned accumulator 24\n",
      "20/12/21 14:10:56 INFO spark.ContextCleaner: Cleaned accumulator 26\n",
      "20/12/21 14:10:56 INFO spark.ContextCleaner: Cleaned accumulator 5\n",
      "20/12/21 14:10:56 INFO spark.ContextCleaner: Cleaned accumulator 21\n",
      "20/12/21 14:10:56 INFO spark.ContextCleaner: Cleaned accumulator 23\n",
      "20/12/21 14:10:56 INFO spark.ContextCleaner: Cleaned accumulator 12\n",
      "20/12/21 14:10:56 INFO datasources.FileSourceStrategy: Pruning directories with: \n",
      "20/12/21 14:10:56 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "20/12/21 14:10:56 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "20/12/21 14:10:56 INFO execution.FileSourceScanExec: Pushed Filters: \n",
      "20/12/21 14:10:56 INFO codegen.CodeGenerator: Code generated in 5.452802 ms\n",
      "20/12/21 14:10:56 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 306.6 KB, free 93.0 MB)\n",
      "20/12/21 14:10:56 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 26.0 KB, free 93.0 MB)\n",
      "20/12/21 14:10:56 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on sparkreadcsv-d00ee42e-1608559809091-driver-svc.default-tenant.svc:7079 (size: 26.0 KB, free: 93.3 MB)\n",
      "20/12/21 14:10:56 INFO spark.SparkContext: Created broadcast 2 from load at NativeMethodAccessorImpl.java:0\n",
      "20/12/21 14:10:56 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4197053 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "20/12/21 14:10:56 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on sparkreadcsv-d00ee42e-1608559809091-driver-svc.default-tenant.svc:7079 in memory (size: 26.0 KB, free: 93.3 MB)\n",
      "20/12/21 14:10:56 INFO spark.ContextCleaner: Cleaned accumulator 32\n",
      "20/12/21 14:10:56 INFO spark.ContextCleaner: Cleaned accumulator 36\n",
      "20/12/21 14:10:56 INFO spark.ContextCleaner: Cleaned accumulator 33\n",
      "20/12/21 14:10:56 INFO spark.ContextCleaner: Cleaned accumulator 34\n",
      "20/12/21 14:10:56 INFO spark.ContextCleaner: Cleaned accumulator 35\n",
      "20/12/21 14:10:56 INFO datasources.FileSourceStrategy: Pruning directories with: \n",
      "20/12/21 14:10:56 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "20/12/21 14:10:56 INFO datasources.FileSourceStrategy: Output Data Schema: struct<length: string, width: string, petallength: string, petalwidth: string, label: string ... 3 more fields>\n",
      "20/12/21 14:10:56 INFO execution.FileSourceScanExec: Pushed Filters: \n",
      "20/12/21 14:10:57 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 306.6 KB, free 93.0 MB)\n",
      "20/12/21 14:10:57 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 26.0 KB, free 93.0 MB)\n",
      "20/12/21 14:10:57 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on sparkreadcsv-d00ee42e-1608559809091-driver-svc.default-tenant.svc:7079 (size: 26.0 KB, free: 93.3 MB)\n",
      "20/12/21 14:10:57 INFO spark.SparkContext: Created broadcast 3 from describe at NativeMethodAccessorImpl.java:0\n",
      "20/12/21 14:10:57 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "20/12/21 14:10:57 INFO spark.SparkContext: Starting job: describe at NativeMethodAccessorImpl.java:0\n",
      "20/12/21 14:10:57 INFO scheduler.DAGScheduler: Registering RDD 13 (describe at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "20/12/21 14:10:57 INFO scheduler.DAGScheduler: Got job 1 (describe at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "20/12/21 14:10:57 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (describe at NativeMethodAccessorImpl.java:0)\n",
      "20/12/21 14:10:57 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)\n",
      "20/12/21 14:10:57 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 1)\n",
      "20/12/21 14:10:57 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[13] at describe at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "20/12/21 14:10:57 INFO spark.ContextCleaner: Cleaned accumulator 37\n",
      "20/12/21 14:10:57 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 27.4 KB, free 92.9 MB)\n",
      "20/12/21 14:10:57 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.7 KB, free 92.9 MB)\n",
      "20/12/21 14:10:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on sparkreadcsv-d00ee42e-1608559809091-driver-svc.default-tenant.svc:7079 (size: 11.7 KB, free: 93.3 MB)\n",
      "20/12/21 14:10:57 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1163\n",
      "20/12/21 14:10:57 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[13] at describe at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "20/12/21 14:10:57 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 1 tasks\n",
      "20/12/21 14:10:57 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 10.200.0.57, executor 2, partition 0, PROCESS_LOCAL, 8248 bytes)\n",
      "20/12/21 14:10:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.200.0.57:45700 (size: 11.7 KB, free: 114.6 MB)\n",
      "20/12/21 14:10:59 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.200.0.57:45700 (size: 26.0 KB, free: 114.6 MB)\n",
      "20/12/21 14:11:03 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 6274 ms on 10.200.0.57 (executor 2) (1/1)\n",
      "20/12/21 14:11:03 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "20/12/21 14:11:03 INFO scheduler.DAGScheduler: ShuffleMapStage 1 (describe at NativeMethodAccessorImpl.java:0) finished in 6.362 s\n",
      "20/12/21 14:11:03 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "20/12/21 14:11:03 INFO scheduler.DAGScheduler: running: Set()\n",
      "20/12/21 14:11:03 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 2)\n",
      "20/12/21 14:11:03 INFO scheduler.DAGScheduler: failed: Set()\n",
      "20/12/21 14:11:03 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (SQLExecutionRDD[16] at describe at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "20/12/21 14:11:03 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 36.3 KB, free 92.9 MB)\n",
      "20/12/21 14:11:03 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 15.1 KB, free 92.9 MB)\n",
      "20/12/21 14:11:03 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on sparkreadcsv-d00ee42e-1608559809091-driver-svc.default-tenant.svc:7079 (size: 15.1 KB, free: 93.2 MB)\n",
      "20/12/21 14:11:03 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1163\n",
      "20/12/21 14:11:03 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (SQLExecutionRDD[16] at describe at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "20/12/21 14:11:03 INFO scheduler.TaskSchedulerImpl: Adding task set 2.0 with 1 tasks\n",
      "20/12/21 14:11:03 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 10.200.0.57, executor 2, partition 0, NODE_LOCAL, 7786 bytes)\n",
      "20/12/21 14:11:03 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.200.0.57:45700 (size: 15.1 KB, free: 114.5 MB)\n",
      "20/12/21 14:11:03 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.200.0.57:58324\n",
      "20/12/21 14:11:04 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 999 ms on 10.200.0.57 (executor 2) (1/1)\n",
      "20/12/21 14:11:04 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "20/12/21 14:11:04 INFO scheduler.DAGScheduler: ResultStage 2 (describe at NativeMethodAccessorImpl.java:0) finished in 1.009 s\n",
      "20/12/21 14:11:04 INFO scheduler.DAGScheduler: Job 1 finished: describe at NativeMethodAccessorImpl.java:0, took 7.392565 s\n",
      "20/12/21 14:11:04 INFO codegen.CodeGenerator: Code generated in 35.95593 ms\n",
      "20/12/21 14:11:04 INFO server.AbstractConnector: Stopped Spark@400ba125{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "20/12/21 14:11:04 INFO ui.SparkUI: Stopped Spark web UI at http://sparkreadcsv-d00ee42e-1608559809091-driver-svc.default-tenant.svc:4040\n",
      "20/12/21 14:11:04 INFO k8s.KubernetesClusterSchedulerBackend: Shutting down all executors\n",
      "20/12/21 14:11:04 INFO k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Asking each executor to shut down\n",
      "20/12/21 14:11:04 WARN k8s.ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed (this is expected if the application is shutting down.)\n",
      "20/12/21 14:11:04 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "20/12/21 14:11:05 INFO memory.MemoryStore: MemoryStore cleared\n",
      "20/12/21 14:11:05 INFO storage.BlockManager: BlockManager stopped\n",
      "20/12/21 14:11:05 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\n",
      "20/12/21 14:11:05 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "20/12/21 14:11:05 INFO spark.SparkContext: Successfully stopped SparkContext\n",
      "20/12/21 14:11:05 INFO util.ShutdownHookManager: Shutdown hook called\n",
      "20/12/21 14:11:05 INFO util.ShutdownHookManager: Deleting directory /var/data/spark-43365362-bc16-4a11-ab39-f4a5f3d188c9/spark-9ecdcfd7-3576-4574-b50f-1881bbe4a577/pyspark-6010c0b2-4626-4251-90e7-4056beca3cc6\n",
      "20/12/21 14:11:05 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-e5c8c7e5-8e98-4b44-8cee-fdde1779df34\n",
      "20/12/21 14:11:05 INFO util.ShutdownHookManager: Deleting directory /var/data/spark-43365362-bc16-4a11-ab39-f4a5f3d188c9/spark-9ecdcfd7-3576-4574-b50f-1881bbe4a577\n",
      "final state: completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dictlist {\n",
       "  background-color: #4EC64B;\n",
       "  text-align: center;\n",
       "  margin: 4px;\n",
       "  border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;}\n",
       ".artifact {\n",
       "  cursor: pointer;\n",
       "  background-color: #4EC64B;\n",
       "  text-align: left;\n",
       "  margin: 4px; border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;\n",
       "}\n",
       "div.block.hidden {\n",
       "  display: none;\n",
       "}\n",
       ".clickable {\n",
       "  cursor: pointer;\n",
       "}\n",
       ".ellipsis {\n",
       "  display: inline-block;\n",
       "  max-width: 60px;\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "}\n",
       ".master-wrapper {\n",
       "  display: flex;\n",
       "  flex-flow: row nowrap;\n",
       "  justify-content: flex-start;\n",
       "  align-items: stretch;\n",
       "}\n",
       ".master-tbl {\n",
       "  flex: 3\n",
       "}\n",
       ".master-wrapper > div {\n",
       "  margin: 4px;\n",
       "  padding: 10px;\n",
       "}\n",
       "iframe.fileview {\n",
       "  border: 0 none;\n",
       "  height: 100%;\n",
       "  width: 100%;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       ".pane-header-title {\n",
       "  width: 80%;\n",
       "  font-weight: 500;\n",
       "}\n",
       ".pane-header {\n",
       "  line-height: 1;\n",
       "  background-color: #4EC64B;\n",
       "  padding: 3px;\n",
       "}\n",
       ".pane-header .close {\n",
       "  font-size: 20px;\n",
       "  font-weight: 700;\n",
       "  float: right;\n",
       "  margin-top: -5px;\n",
       "}\n",
       ".master-wrapper .right-pane {\n",
       "  border: 1px inset silver;\n",
       "  width: 40%;\n",
       "  min-height: 300px;\n",
       "  flex: 3\n",
       "  min-width: 500px;\n",
       "}\n",
       ".master-wrapper * {\n",
       "  box-sizing: border-box;\n",
       "}\n",
       "</style><script>\n",
       "function copyToClipboard(fld) {\n",
       "    if (document.queryCommandSupported && document.queryCommandSupported('copy')) {\n",
       "        var textarea = document.createElement('textarea');\n",
       "        textarea.textContent = fld.innerHTML;\n",
       "        textarea.style.position = 'fixed';\n",
       "        document.body.appendChild(textarea);\n",
       "        textarea.select();\n",
       "\n",
       "        try {\n",
       "            return document.execCommand('copy'); // Security exception may be thrown by some browsers.\n",
       "        } catch (ex) {\n",
       "\n",
       "        } finally {\n",
       "            document.body.removeChild(textarea);\n",
       "        }\n",
       "    }\n",
       "}\n",
       "function expandPanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName');\n",
       "  console.log(el.title);\n",
       "\n",
       "  document.querySelector(panelName + \"-title\").innerHTML = el.title\n",
       "  iframe = document.querySelector(panelName + \"-body\");\n",
       "\n",
       "  const tblcss = `<style> body { font-family: Arial, Helvetica, sans-serif;}\n",
       "    #csv { margin-bottom: 15px; }\n",
       "    #csv table { border-collapse: collapse;}\n",
       "    #csv table td { padding: 4px 8px; border: 1px solid silver;} </style>`;\n",
       "\n",
       "  function csvToHtmlTable(str) {\n",
       "    return '<div id=\"csv\"><table><tr><td>' +  str.replace(/[\\n\\r]+$/g, '').replace(/[\\n\\r]+/g, '</td></tr><tr><td>')\n",
       "      .replace(/,/g, '</td><td>') + '</td></tr></table></div>';\n",
       "  }\n",
       "\n",
       "  function reqListener () {\n",
       "    if (el.title.endsWith(\".csv\")) {\n",
       "      iframe.setAttribute(\"srcdoc\", tblcss + csvToHtmlTable(this.responseText));\n",
       "    } else {\n",
       "      iframe.setAttribute(\"srcdoc\", this.responseText);\n",
       "    }\n",
       "    console.log(this.responseText);\n",
       "  }\n",
       "\n",
       "  const oReq = new XMLHttpRequest();\n",
       "  oReq.addEventListener(\"load\", reqListener);\n",
       "  oReq.open(\"GET\", el.title);\n",
       "  oReq.send();\n",
       "\n",
       "\n",
       "  //iframe.src = el.title;\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.remove(\"hidden\");\n",
       "  }\n",
       "}\n",
       "function closePanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName')\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (!resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.add(\"hidden\");\n",
       "  }\n",
       "}\n",
       "\n",
       "</script>\n",
       "<div class=\"master-wrapper\">\n",
       "  <div class=\"block master-tbl\"><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>project</th>\n",
       "      <th>uid</th>\n",
       "      <th>iter</th>\n",
       "      <th>start</th>\n",
       "      <th>state</th>\n",
       "      <th>name</th>\n",
       "      <th>labels</th>\n",
       "      <th>inputs</th>\n",
       "      <th>parameters</th>\n",
       "      <th>results</th>\n",
       "      <th>artifacts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>default</td>\n",
       "      <td><div title=\"ddb40aabe2ca48a78b9b09437c4b8ef6\"><a href=\"https://mlrun-ui.default-tenant.app.hsbctesting3.iguazio-cd0.com/projects/default/jobs/monitor/ddb40aabe2ca48a78b9b09437c4b8ef6/info\" target=\"_blank\" >...7c4b8ef6</a></div></td>\n",
       "      <td>0</td>\n",
       "      <td>Dec 21 14:10:33</td>\n",
       "      <td>completed</td>\n",
       "      <td>sparkreadcsv</td>\n",
       "      <td><div class=\"dictlist\">v3io_user=admin</div><div class=\"dictlist\">kind=spark</div><div class=\"dictlist\">owner=admin</div><div class=\"dictlist\">mlrun/job=sparkreadcsv-d00ee42e</div><div class=\"dictlist\">host=sparkreadcsv-d00ee42e-driver</div></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td><div class=\"artifact\" onclick=\"expandPanel(this)\" paneName=\"result3171e130\" title=\"/files/df_sample.csv\">df_sample</div></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></div>\n",
       "  <div id=\"result3171e130-pane\" class=\"right-pane block hidden\">\n",
       "    <div class=\"pane-header\">\n",
       "      <span id=\"result3171e130-title\" class=\"pane-header-title\">Title</span>\n",
       "      <span onclick=\"closePanel(this)\" paneName=\"result3171e130\" class=\"close clickable\">&times;</span>\n",
       "    </div>\n",
       "    <iframe class=\"fileview\" id=\"result3171e130-body\"></iframe>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to track results use .show() or .logs() or in CLI: \n",
      "!mlrun get run ddb40aabe2ca48a78b9b09437c4b8ef6 --project default , !mlrun logs ddb40aabe2ca48a78b9b09437c4b8ef6 --project default\n",
      "> 2020-12-21 14:11:11,513 [info] run executed, status=completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlrun.model.RunObject at 0x7ff9a28e1790>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlrun\n",
    "import os\n",
    "\n",
    "# set up new spark function with spark operator\n",
    "# command will use our spark code which needs to be located on our file system\n",
    "# the name param can have only non capital letters (k8s convention)\n",
    "sj = mlrun.new_function(kind='spark', command='/User/sparkreadCSV.py', name='sparkreadcsv') \n",
    "\n",
    "# set spark driver config (gpu_type & gpus=<number_of_gpus>  supported too)\n",
    "sj.with_driver_limits(cpu=\"1300m\")\n",
    "sj.with_driver_requests(cpu=1, mem=\"512m\") \n",
    "\n",
    "# set spark executor config (gpu_type & gpus=<number_of_gpus> are supported too)\n",
    "sj.with_executor_limits(cpu=\"1400m\")\n",
    "sj.with_executor_requests(cpu=1, mem=\"512m\")\n",
    "\n",
    "# adds fuse, daemon & iguazio's jars support\n",
    "sj.with_igz_spark() \n",
    "\n",
    "# args are also supported\n",
    "sj.spec.args = ['-spark.eventLog.enabled','true']\n",
    "\n",
    "# add python module\n",
    "sj.spec.build.commands = ['pip install matplotlib']\n",
    "\n",
    "# Number of executors\n",
    "sj.spec.replicas = 2 \n",
    "\n",
    "# Rebuilds the image with MLRun - needed in order to support artifactlogging etc\n",
    "sj.deploy() \n",
    "\n",
    "# Run task while setting the artifact path on which our run artifact (in any) will be saved\n",
    "sj.run(artifact_path='/User')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
