{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognition Using mlrun with OpenCV And PyTorch\n",
    " A complete pipeline of data processing, model training and serving function deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install mlrun and kubeflow pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/mlrun/mlrun.git@development\n",
    "# !pip install kfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restart jupyter kernel after initial installations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies for the code and set config \n",
    "\n",
    "It is possible that after installing dependencies locally, you will need to restart Jupyter kernel to successfully import the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "import nuclio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change following magic command to %%nuclio cmd -c if the following packages are already installed locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%nuclio cmd\n",
    "pip install scikit-build\n",
    "pip install cmake==3.13.3\n",
    "pip install face_recognition\n",
    "pip install opencv-contrib-python\n",
    "pip install imutils\n",
    "pip install torch torchvision \n",
    "pip install pandas\n",
    "pip install v3io_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nuclio config spec.build.baseImage = \"python:3.6-jessie\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare global variables and perform necessary imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/User/demos/demos/faces/dataset/'\n",
    "ARTIFACTS_PATH = '/User/demos/demos/faces/artifacts/'\n",
    "MODELS_PATH = '/User/demos/demos/faces/models.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "from urllib.request import urlopen\n",
    "from io import BytesIO\n",
    "import cv2\n",
    "import face_recognition\n",
    "from imutils import paths\n",
    "from pickle import load, dump\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from mlrun.artifacts import TableArtifact\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random\n",
    "import string\n",
    "import v3io_frames as v3f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and define mlrun functions for the pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "from mlrun import new_function, code_to_function, NewTask, mount_v3io\n",
    "import kfp\n",
    "from kfp import dsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_images(context, cuda=True):\n",
    "    \n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    context.logger.info(f'Running on device: {device}')\n",
    "    \n",
    "    client = v3f.Client(\"framesd:8081\", container=\"users\")\n",
    "    \n",
    "    if not os.path.exists(DATA_PATH + 'processed'):\n",
    "        os.makedirs(DATA_PATH + 'processed')\n",
    "    \n",
    "    if not os.path.exists(DATA_PATH + 'label_pending'):\n",
    "        os.makedirs(DATA_PATH + 'label_pending')\n",
    "    \n",
    "    # If no train images exist in the predefined path we will train the model on a small dataset of movie actresses\n",
    "    if not os.path.exists(DATA_PATH + 'input'):\n",
    "        os.makedirs(DATA_PATH + 'input')\n",
    "        resp = urlopen('https://iguazio-public.s3.amazonaws.com/roy-actresses/Actresses.zip')\n",
    "        zip_ref = zipfile.ZipFile(BytesIO(resp.read()), 'r')\n",
    "        zip_ref.extractall(DATA_PATH + 'input')\n",
    "        zip_ref.close()\n",
    "    \n",
    "    if os.path.exists(DATA_PATH + 'input/__MACOSX'):\n",
    "        shutil.rmtree(DATA_PATH + 'input/__MACOSX')\n",
    "    \n",
    "    idx_file_path = ARTIFACTS_PATH+\"idx2name.csv\"\n",
    "    if os.path.exists(idx_file_path):\n",
    "        idx2name_df = pd.read_csv(idx_file_path)\n",
    "    else:\n",
    "        idx2name_df = pd.DataFrame(columns=['value', 'name'])\n",
    "    \n",
    "    #creates a mapping of classes(person's names) to target value\n",
    "    new_classes_names = [f for f in os.listdir(DATA_PATH + 'input') if not '.ipynb' in f and f not in idx2name_df['name'].values]\n",
    "    \n",
    "    initial_len = len(idx2name_df)\n",
    "    final_len = len(idx2name_df) + len(new_classes_names)\n",
    "    for i in range(initial_len, final_len):\n",
    "        idx2name_df.loc[i] = {'value': i, 'name': new_classes_names.pop()}\n",
    "    \n",
    "    name2idx = idx2name_df.set_index('name')['value'].to_dict()\n",
    "    \n",
    "    #log name to index mapping into mlrun context\n",
    "    context.log_artifact(TableArtifact('idx2name', df=idx2name_df), target_path='idx2name.csv')\n",
    "    \n",
    "    #generates a list of paths to labeled images \n",
    "    imagePaths = [f for f in paths.list_images(DATA_PATH + 'input') if not '.ipynb' in f]\n",
    "    knownEncodings = []\n",
    "    knownLabels = []\n",
    "    fileNames = []\n",
    "    urls = []\n",
    "    for (i, imagePath) in enumerate(imagePaths):\n",
    "        print(\"[INFO] processing image {}/{}\".format(i + 1, len(imagePaths)))\n",
    "        #extracts label (person's name) of the image\n",
    "        name = imagePath.split(os.path.sep)[-2]\n",
    "        \n",
    "        #prepares to relocate image after extracting features\n",
    "        file_name = imagePath.split(os.path.sep)[-1]\n",
    "        new_path = DATA_PATH + 'processed/' + file_name\n",
    "        \n",
    "        #converts image format to RGB for comptability with face_recognition library\n",
    "        image = cv2.imread(imagePath)\n",
    "        rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        #detects coordinates of faces bounding boxes\n",
    "        boxes = face_recognition.face_locations(rgb, model='hog')\n",
    "        \n",
    "        #computes embeddings for detected faces\n",
    "        encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "        \n",
    "        #this code assumes that a person's folder in the dataset does not contain an image with a face other then his own\n",
    "        for enc in encodings:\n",
    "            file_name = name + '_' + ''.join(random.choices(string.ascii_uppercase + string.digits, k=5))                                                           \n",
    "            knownEncodings.append(enc)\n",
    "            knownLabels.append([name2idx[name]])\n",
    "            fileNames.append(file_name)\n",
    "            urls.append(new_path)\n",
    "        \n",
    "        #move image to processed images directory\n",
    "        shutil.move(imagePath, new_path)\n",
    "        \n",
    "    #saves computed encodings to avoid repeating computations\n",
    "    df_x = pd.DataFrame(knownEncodings, columns=['c' + str(i).zfill(3) for i in range(128)]).reset_index(drop=True)\n",
    "    df_y = pd.DataFrame(knownLabels, columns=['label']).reset_index(drop=True)\n",
    "    df_details = pd.DataFrame([['initial training']*3]*len(df_x), columns=['imgUrl', 'camera', 'time'])\n",
    "    df_details['time'] = [datetime.datetime.utcnow()]*len(df_x)\n",
    "    df_details['imgUrl'] = urls\n",
    "    data_df = pd.concat([df_x, df_y, df_details], axis=1)\n",
    "    data_df['fileName'] = fileNames\n",
    "    \n",
    "    client.write(backend='kv', table='iguazio/demos/demos/faces/artifacts/encodings', dfs=data_df, index_cols=['fileName'])\n",
    "    \n",
    "    with open('encodings_path.txt', 'w+') as f:\n",
    "        f.write('iguazio/demos/demos/faces/artifacts/encodings')\n",
    "    context.log_artifact('encodings_path', src_path=f.name, target_path=f.name)\n",
    "    os.remove('encodings_path.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(context, processed_data, model_name='model.bst', cuda=True):\n",
    "    \n",
    "    if cuda:\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "            context.logger.info(f\"Running on cuda device: {device}\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "            context.logger.info(\"Requested running on cuda but no cuda device available.\\nRunning on cpu\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    \n",
    "    # prepare data from training\n",
    "    context.logger.info('Client')\n",
    "    client = v3f.Client('framesd:8081', container=\"users\")\n",
    "    with open(processed_data.url, 'r') as f:                      \n",
    "        t = f.read()\n",
    "        \n",
    "    data_df = client.read(backend=\"kv\", table=t, reset_index=False, filter='label != -1')\n",
    "    X = data_df[['c'+str(i).zfill(3) for i in range(128)]].values\n",
    "    y = data_df['label'].values\n",
    "    \n",
    "    n_classes = len(set(y))\n",
    "    \n",
    "    X = torch.as_tensor(X, device=device)\n",
    "    y = torch.tensor(y, device=device).reshape(-1, 1)\n",
    "    \n",
    "    input_dim = 128\n",
    "    hidden_dim = 64\n",
    "    output_dim = n_classes\n",
    "    \n",
    "    spec = importlib.util.spec_from_file_location('models', MODELS_PATH)\n",
    "    models = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(models)\n",
    "    \n",
    "    model = models.FeedForwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "    model.to(device)\n",
    "    model = model.double()\n",
    "    \n",
    "    # define loss and optimizer for the task\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    learning_rate = 0.05\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    \n",
    "    # train the network\n",
    "    n_iters = X.size(0) * 5\n",
    "    for i in range(n_iters):\n",
    "        r = random.randint(0, X.size(0) - 1)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(X[r]).reshape(1, -1)\n",
    "        loss = criterion(out, y[r])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    context.logger.info('Save model')\n",
    "    #saves and logs model into mlrun context\n",
    "    dump(model._modules, open(model_name, 'wb'))\n",
    "    context.log_artifact('model', src_path=model_name, target_path=model_name, labels={'framework': 'Pytorch-FeedForwardNN'})\n",
    "    os.remove(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.function.RemoteRuntime at 0x7f61f5886d68>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# serving function\n",
    "serving_function = code_to_function(name='recognize-faces', \n",
    "                                      filename='./nuclio-face-prediction.ipynb',\n",
    "                                      runtime='nuclio')\n",
    "\n",
    "serving_function.with_http(workers=2).apply(mount_v3io())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kind: remote\n",
      "metadata:\n",
      "  name: recognize-faces\n",
      "  tag: ''\n",
      "  project: ''\n",
      "spec:\n",
      "  command: ''\n",
      "  args: []\n",
      "  image: ''\n",
      "  description: ''\n",
      "  volumes:\n",
      "  - flexVolume:\n",
      "      driver: v3io/fuse\n",
      "      options:\n",
      "        accessKey: 33eb59ef-9d82-4260-8175-02e16a20cfc9\n",
      "        container: users\n",
      "        subPath: /iguazio\n",
      "    name: v3io\n",
      "  volume_mounts:\n",
      "  - mountPath: /User\n",
      "    name: v3io\n",
      "  env:\n",
      "  - name: V3IO_API\n",
      "    value: v3io-webapi.default-tenant.svc:8081\n",
      "  - name: V3IO_USERNAME\n",
      "    value: iguazio\n",
      "  - name: V3IO_ACCESS_KEY\n",
      "    value: 33eb59ef-9d82-4260-8175-02e16a20cfc9\n",
      "  config:\n",
      "    spec.triggers.http:\n",
      "      kind: http\n",
      "      maxWorkers: 2\n",
      "      attributes:\n",
      "        ingresses: {}\n",
      "      annotations: {}\n",
      "  base_spec:\n",
      "    apiVersion: nuclio.io/v1\n",
      "    kind: Function\n",
      "    metadata:\n",
      "      annotations:\n",
      "        nuclio.io/generated_by: function generated at 23-02-2020 by iguazio from ./nuclio-face-prediction.ipynb\n",
      "      labels: {}\n",
      "      name: recognize-faces\n",
      "    spec:\n",
      "      build:\n",
      "        baseImage: python:3.6-jessie\n",
      "        commands:\n",
      "        - pip install opencv-contrib-python\n",
      "        - pip install imutils\n",
      "        - pip install torch torchvision\n",
      "        - pip install pandas\n",
      "        - pip install v3io_frames\n",
      "        - pip install scikit-build\n",
      "        - pip install cmake==3.13.3\n",
      "        - pip install face_recognition\n",
      "        functionSourceCode: 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\n",
      "        noBaseImagesPull: true\n",
      "      env:\n",
      "      - name: MODELS_PATH\n",
      "        value: /User/demos/demos/faces/models.py\n",
      "      - name: MODEL_PATH\n",
      "        value: /User/demos/demos/faces/artifacts/model.bst\n",
      "      - name: DATA_PATH\n",
      "        value: /User/demos/demos/faces/dataset/\n",
      "      - name: CLASSES_MAP\n",
      "        value: /User/demos/demos/faces/artifacts/idx2name.csv\n",
      "      - name: V3IO_ACCESS_KEY\n",
      "        value: 33eb59ef-9d82-4260-8175-02e16a20cfc9\n",
      "      handler: nuclio-face-prediction:handler\n",
      "      runtime: python:3.6\n",
      "      volumes: []\n",
      "  source: ''\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(serving_function.to_yaml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test pipeline functions locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "task = NewTask(handler=encode_images, out_path=ARTIFACTS_PATH)\n",
    "run = new_function().run(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task2 = NewTask(handler=train, inputs={'processed_data': run.outputs['encodings_path']}, out_path=ARTIFACTS_PATH)\n",
    "train = new_function().run(task2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function from notebook and build image\n",
    "supposed to take a few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = code_to_function('face-recognition', runtime='job')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mlrun] 2020-02-23 15:43:32,395 database connection is not configured\n",
      "[mlrun] 2020-02-23 15:43:32,396 building image (.mlrun/func-default-face-recognition-latest)\n",
      "FROM python:3.6-jessie\n",
      "RUN pip install scikit-build\n",
      "RUN pip install cmake==3.13.3\n",
      "RUN pip install face_recognition\n",
      "RUN pip install opencv-contrib-python\n",
      "RUN pip install imutils\n",
      "RUN pip install torch torchvision\n",
      "RUN pip install pandas\n",
      "RUN pip install v3io_frames\n",
      "RUN pip install mlrun\n",
      "\n",
      "[mlrun] 2020-02-23 15:43:32,398 using in-cluster config.\n",
      "[mlrun] 2020-02-23 15:43:32,416 Pod mlrun-build-face-recognition-hvlh6 created\n",
      "..\n",
      "\u001b[36mINFO\u001b[0m[0000] Resolved base name python:3.6-jessie to python:3.6-jessie \n",
      "\u001b[36mINFO\u001b[0m[0000] Resolved base name python:3.6-jessie to python:3.6-jessie \n",
      "\u001b[36mINFO\u001b[0m[0000] Downloading base image python:3.6-jessie     \n",
      "\u001b[36mINFO\u001b[0m[0000] Error while retrieving image from cache: getting file info: stat /cache/sha256:0318d80cb241983eda20b905d77fa0bfb06e29e5aabf075c7941ea687f1c125a: no such file or directory \n",
      "\u001b[36mINFO\u001b[0m[0000] Downloading base image python:3.6-jessie     \n",
      "\u001b[36mINFO\u001b[0m[0000] Built cross stage deps: map[]                \n",
      "\u001b[36mINFO\u001b[0m[0000] Downloading base image python:3.6-jessie     \n",
      "\u001b[36mINFO\u001b[0m[0000] Error while retrieving image from cache: getting file info: stat /cache/sha256:0318d80cb241983eda20b905d77fa0bfb06e29e5aabf075c7941ea687f1c125a: no such file or directory \n",
      "\u001b[36mINFO\u001b[0m[0000] Downloading base image python:3.6-jessie     \n",
      "\u001b[36mINFO\u001b[0m[0001] Unpacking rootfs as cmd RUN pip install scikit-build requires it. \n",
      "\u001b[36mINFO\u001b[0m[0011] Taking snapshot of full filesystem...        \n",
      "\u001b[36mINFO\u001b[0m[0022] RUN pip install scikit-build                 \n",
      "\u001b[36mINFO\u001b[0m[0022] cmd: /bin/sh                                 \n",
      "\u001b[36mINFO\u001b[0m[0022] args: [-c pip install scikit-build]          \n",
      "Collecting scikit-build\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/b5/c6ca60421991c22e69b9a950b0d046e06d714f79f7071946ab885c7115fb/scikit_build-0.10.0-py2.py3-none-any.whl (66kB)\n",
      "Collecting packaging (from scikit-build)\n",
      "  Downloading https://files.pythonhosted.org/packages/98/42/87c585dd3b113c775e65fd6b8d9d0a43abe1819c471d7af702d4e01e9b20/packaging-20.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: wheel>=0.29.0 in /usr/local/lib/python3.6/site-packages (from scikit-build) (0.33.4)\n",
      "Requirement already satisfied: setuptools>=28.0.0 in /usr/local/lib/python3.6/site-packages (from scikit-build) (41.0.1)\n",
      "Collecting pyparsing>=2.0.2 (from packaging->scikit-build)\n",
      "  Downloading https://files.pythonhosted.org/packages/5d/bc/1e58593167fade7b544bfe9502a26dc860940a79ab306e651e7f13be68c2/pyparsing-2.4.6-py2.py3-none-any.whl (67kB)\n",
      "Collecting six (from packaging->scikit-build)\n",
      "  Downloading https://files.pythonhosted.org/packages/65/eb/1f97cb97bfc2390a276969c6fae16075da282f5058082d4cb10c6c5c1dba/six-1.14.0-py2.py3-none-any.whl\n",
      "Installing collected packages: pyparsing, six, packaging, scikit-build\n",
      "Successfully installed packaging-20.1 pyparsing-2.4.6 scikit-build-0.10.0 six-1.14.0\n",
      "WARNING: You are using pip version 19.1.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "\u001b[36mINFO\u001b[0m[0024] Taking snapshot of full filesystem...        \n",
      "\u001b[36mINFO\u001b[0m[0026] RUN pip install cmake==3.13.3                \n",
      "\u001b[36mINFO\u001b[0m[0026] cmd: /bin/sh                                 \n",
      "\u001b[36mINFO\u001b[0m[0026] args: [-c pip install cmake==3.13.3]         \n",
      "Collecting cmake==3.13.3\n",
      "  Downloading https://files.pythonhosted.org/packages/45/c4/e69313ade2a3e992e7178744b0e56bdd8f23e79e15066a68cf490504beed/cmake-3.13.3-cp36-cp36m-manylinux1_x86_64.whl (15.9MB)\n",
      "Installing collected packages: cmake\n",
      "Successfully installed cmake-3.13.3\n",
      "WARNING: You are using pip version 19.1.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "\u001b[36mINFO\u001b[0m[0028] Taking snapshot of full filesystem...        \n",
      "\u001b[36mINFO\u001b[0m[0031] RUN pip install face_recognition             \n",
      "\u001b[36mINFO\u001b[0m[0031] cmd: /bin/sh                                 \n",
      "\u001b[36mINFO\u001b[0m[0031] args: [-c pip install face_recognition]      \n",
      "Collecting face_recognition\n",
      "  Downloading https://files.pythonhosted.org/packages/1e/95/f6c9330f54ab07bfa032bf3715c12455a381083125d8880c43cbe76bb3d0/face_recognition-1.3.0-py2.py3-none-any.whl\n",
      "Collecting face-recognition-models>=0.3.0 (from face_recognition)\n",
      "  Downloading https://files.pythonhosted.org/packages/cf/3b/4fd8c534f6c0d1b80ce0973d01331525538045084c73c153ee6df20224cf/face_recognition_models-0.3.0.tar.gz (100.1MB)\n",
      "Collecting Click>=6.0 (from face_recognition)\n",
      "  Downloading https://files.pythonhosted.org/packages/fa/37/45185cb5abbc30d7257104c434fe0b07e5a195a6847506c074527aa599ec/Click-7.0-py2.py3-none-any.whl (81kB)\n",
      "Collecting dlib>=19.7 (from face_recognition)\n",
      "  Downloading https://files.pythonhosted.org/packages/63/92/05c3b98636661cb80d190a5a777dd94effcc14c0f6893222e5ca81e74fbc/dlib-19.19.0.tar.gz (3.2MB)\n",
      "Collecting Pillow (from face_recognition)\n",
      "  Downloading https://files.pythonhosted.org/packages/19/5e/23dcc0ce3cc2abe92efd3cd61d764bee6ccdf1b667a1fb566f45dc249953/Pillow-7.0.0-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
      "Collecting numpy (from face_recognition)\n",
      "  Downloading https://files.pythonhosted.org/packages/62/20/4d43e141b5bc426ba38274933ef8e76e85c7adea2c321ecf9ebf7421cedf/numpy-1.18.1-cp36-cp36m-manylinux1_x86_64.whl (20.1MB)\n",
      "Building wheels for collected packages: face-recognition-models, dlib\n",
      "  Building wheel for face-recognition-models (setup.py): started\n",
      "  Building wheel for face-recognition-models (setup.py): finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/d2/99/18/59c6c8f01e39810415c0e63f5bede7d83dfb0ffc039865465f\n",
      "  Building wheel for dlib (setup.py): started\n"
     ]
    }
   ],
   "source": [
    "fn.deploy()\n",
    "#fn.with_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.kubejob.KubejobRuntime at 0x7f4fdcfd8198>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlrun import mlconf\n",
    "mlconf.dbpath = 'http://mlrun-api:8080'\n",
    "fn.apply(mount_v3io())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the lines below based on free GPUs. If you wish to utilize a GPU during training process uncomment the first. If you wish to utilize a GPU for prediction uncomment the latter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fn.gpus(1)\n",
    "#serving_function.gpus(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='face recognition pipeline',\n",
    "    description='Creates and deploys a face recognition model'\n",
    ")\n",
    "def face_recognition_pipeline(with_cuda=True):\n",
    "    \n",
    "    encode = fn.as_step(name='encode-images', handler='encode_images', out_path=ARTIFACTS_PATH, outputs=['idx2name', 'encodings_path'],\n",
    "                       inputs={'cuda': with_cuda})\n",
    "    \n",
    "    train = fn.as_step(name='train', handler='train', out_path=ARTIFACTS_PATH, outputs=['model'], \n",
    "                               inputs={'processed_data': encode.outputs['encodings_path'], 'cuda': with_cuda})\n",
    "    \n",
    "    deploy = serving_function.deploy_step(project='default', models={'face_rec_v1': train.outputs['model']})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client(namespace='default-tenant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For debug purposes compile pipeline code\n",
    "kfp.compiler.Compiler().compile(face_recognition_pipeline, 'face_rec.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = {}\n",
    "run_result = client.create_run_from_pipeline_func(face_recognition_pipeline, arguments=arguments, run_name='face_rec_1', experiment_name='face_rec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
